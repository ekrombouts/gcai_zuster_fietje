{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekrombouts/gcai_zuster_fietje/blob/main/notebooks/300_GenCareAISAMPCDatasetCreation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwx-li06oX8C"
      },
      "source": [
        "## Creating a SAMPC Dataset from GenCareAI Client records\n",
        "\n",
        "**Author:** Eva Rombouts  \n",
        "**Date:** 2024-09-16  \n",
        "**Updated:** 2024-10-16\n",
        "\n",
        "### Description\n",
        "This notebook summarizes nursing home client notes into the SAMPC format using OpenAI’s GPT model via LangChain. It processes the data, generates summaries, and prepares the dataset by splitting it into training, validation, and test sets, and uploads it to the Hugging Face Hub for use in machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup and Library Imports"
      ],
      "metadata": {
        "id": "EIddGTJcE7Hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install GenCareAIUtils\n",
        "from GenCareAIUtils import GenCareAISetup\n",
        "\n",
        "setup = GenCareAISetup()\n",
        "\n",
        "if setup.environment == 'Colab':\n",
        "    !pip install -q datasets langchain langchain_community langchain_openai\n",
        "\n",
        "verbose = True"
      ],
      "metadata": {
        "id": "ufty-17OFS0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OOrSISU7r15"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm  # Progress bar\n",
        "from datasets import load_dataset, Dataset, DatasetDict  # For loading and managing datasets\n",
        "from typing import List\n",
        "\n",
        "# Langchain modules\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_community.callbacks import get_openai_callback\n",
        "\n",
        "# Pydantic library for data validation\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# OpenAI API integration using langchain\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Scikit-learn library for splitting datasets into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQEKWDLNv1QJ"
      },
      "outputs": [],
      "source": [
        "# Set parameters\n",
        "seed = 6\n",
        "\n",
        "# Data paths\n",
        "nursing_care_home_name = \"Olympia\"\n",
        "# For reading data\n",
        "path_hf_records = f\"ekrombouts/{nursing_care_home_name}_records\"\n",
        "path_hf_clients = f\"ekrombouts/{nursing_care_home_name}_clients\"\n",
        "\n",
        "# For writing data\n",
        "path_hf_sampc = f\"ekrombouts/{nursing_care_home_name}_SAMPC_dataset\"\n",
        "commit_message = \"SAMPC dataset\"\n",
        "\n",
        "# File path for saving the generated SAMPC summaries\n",
        "fn_responses = setup.get_file_path(f\"data/care_pal/{nursing_care_home_name}_SAMPC_dataset.pkl\")\n",
        "\n",
        "# Settings for SAMPC summary generation\n",
        "model = \"gpt-4o-mini-2024-07-18\"\n",
        "temperature = 0.3\n",
        "\n",
        "sep_line = 50 * '-'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Preprocessing Data\n",
        "Client records and notes from fictional clients of a nursing home are loaded, cleaned, and processed. The notes are grouped by week"
      ],
      "metadata": {
        "id": "FoCTuwkGJZov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset from Hugging Face and preprocess\n",
        "dataset = load_dataset(path_hf_records)\n",
        "df_records = dataset['train'].to_pandas()\n",
        "\n",
        "# Floor datetime to the first day of the month\n",
        "df_records['week'] = df_records['datetime'].dt.to_period('W').dt.to_timestamp()\n",
        "\n",
        "# Group records by 'client_id' and 'month', concatenating notes into one string\n",
        "df = (df_records\n",
        "    .dropna()\n",
        "    .assign(week=lambda df: pd.to_datetime(df['datetime']).dt.to_period('W').dt.to_timestamp()) # Add 'week' column\n",
        "    .groupby(['client_id', 'week'])\n",
        "    .agg({'note': lambda x: '\\n'.join(x)}) # Concatenate 'note' values\n",
        "    .reset_index()\n",
        "    .rename(columns={'note': 'weeknotes'})\n",
        ")\n",
        "\n",
        "if verbose:\n",
        "  print(f\"Rows in original df: {df_records.shape[0]}, rows in processed df: {df.shape[0]}\\n\")\n",
        "  print(f\"SAMPLES{sep_line}\\n{df.sample(3)}\\n\")\n",
        "  print(f\"\\nContext column (weeknotes) example:{sep_line}\\n{df['weeknotes'].iloc[0]}\")\n"
      ],
      "metadata": {
        "id": "0Vc3BWewlask"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM Response Generation: SAMPC Summaries"
      ],
      "metadata": {
        "id": "aMgbJRRcVB0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the SAMPC model using Pydantic to structure the summarised data\n",
        "class SAMPC(BaseModel):\n",
        "    somatiek: List[str] = Field(description=\"lichamelijke klachten\")\n",
        "    adl: str = Field(description=\"beschrijf welke hulp de cliënt nodig heeft bij wassen en kleden\")\n",
        "    mobiliteit: str = Field(description=\"beschrijf de mobiliteit (bv rolstoelafhankelijk, gebruik rollator, valgevaar)\")\n",
        "    continentie: str = Field(description=\"continentie\")\n",
        "    maatschappelijk: str = Field(description=\"beschrijf bijzonderheden familie en dagbesteding\")\n",
        "    psychisch: List[str] = Field(description=\"beschrijf cognitie en probleemgedrag\")\n",
        "\n",
        "\n",
        "# Set up a parser to handle the output and inject instructions into the prompt template\n",
        "pyd_parser = PydanticOutputParser(pydantic_object=SAMPC)\n"
      ],
      "metadata": {
        "id": "IOAyIZuCU4By"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template=\"\"\"\n",
        "Vat de onderstaande rapportages kort en bondig samen om een profiel van de cliënt te schetsen met de volgende categorieën:\n",
        "\n",
        "Categorieën:\n",
        "- Somatiek\n",
        "- Wassen en aankleden\n",
        "- Mobiliteit\n",
        "- Continentie\n",
        "- Maatschappelijk\n",
        "- Psychisch\n",
        "\n",
        "Belangrijk:\n",
        "- Gebruik uitsluitend informatie uit de rapportages. Voeg geen eigen interpretaties toe.\n",
        "- Als er geen informatie beschikbaar is voor een categorie, noteer dan 'geen informatie beschikbaar' voor die categorie.\n",
        "- Richt je op algemene observaties en patronen, zonder de details van de rapportages over te nemen.\n",
        "\n",
        "---\n",
        "RAPPORTAGES:\n",
        "{rapportages}\n",
        "---\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"rapportages\"],\n",
        "    partial_variables={\"format_instructions\": pyd_parser.get_format_instructions()},\n",
        ")\n"
      ],
      "metadata": {
        "id": "POwEfuzncjyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize OpenAI Chat model\n",
        "llm = ChatOpenAI(\n",
        "    api_key=setup.get_openai_key(),\n",
        "    model=model,\n",
        "    temperature=temperature\n",
        ")\n",
        "\n",
        "chain = prompt_template | llm | pyd_parser\n",
        "\n",
        "\n",
        "if verbose:\n",
        "    sample_id = 50\n",
        "    sample_context = df['weeknotes'].iloc[sample_id]\n",
        "\n",
        "    sample_prompt = template.format(\n",
        "            rapportages=sample_context,\n",
        "            format_instructions=pyd_parser.get_format_instructions()\n",
        "    )\n",
        "\n",
        "    result = chain.invoke({\"rapportages\": sample_context})\n",
        "\n",
        "    print(sample_prompt)\n",
        "    print(\"RESPONSE\")\n",
        "    print(\"Somatiek:\\t\", result.somatiek)\n",
        "    print(\"ADL:\\t\\t\", result.adl)\n",
        "    print(\"Mobiliteit:\\t\", result.mobiliteit)\n",
        "    print(\"Continentie:\\t\", result.continentie)\n",
        "    print(\"Maatschappelijk:\", result.maatschappelijk)\n",
        "    print(\"Psychisch:\\t\", result.psychisch)\n"
      ],
      "metadata": {
        "id": "sWTHsN8KYkIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Function to generate the SAMPC summary\n",
        "# def generate_sampc_summary(notes: str) -> SAMPC:\n",
        "#     result = chain.invoke({\"rapportages\": notes})\n",
        "#     return result\n",
        "\n",
        "# # Generate Summaries\n",
        "# sampc_results = []\n",
        "\n",
        "# with get_openai_callback() as cb:\n",
        "#   # Loop through the 'notes' column and generate SAMPC summaries, store them in the list\n",
        "#   for notes in tqdm(df['notes'], desc=\"Generating SAMPC summaries\"):\n",
        "#       sampc_summary = generate_sampc_summary(notes)\n",
        "#       sampc_results.append(sampc_summary)\n",
        "#   print(cb)"
      ],
      "metadata": {
        "id": "9_navcQxdjgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "CjAVufmKnuhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate the SAMPC summary with error handling\n",
        "def generate_sampc_summary(notes: str) -> SAMPC:\n",
        "    try:\n",
        "        result = chain.invoke({\"rapportages\": notes})\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating SAMPC summary: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load the previously saved dataframe if it exists, otherwise start fresh\n",
        "if os.path.exists(fn_responses):\n",
        "    df = pd.read_pickle(fn_responses)\n",
        "else:\n",
        "    df['sampc_response'] = None  # Ensure the column exists\n",
        "\n",
        "# Create a callback instance to track cost\n",
        "with get_openai_callback() as cb:\n",
        "\n",
        "    # Generate Summaries, process only new entries\n",
        "    with tqdm(total=len(df), desc=\"Generating SAMPC summaries\") as pbar:  # Set the total for the progress bar\n",
        "        for idx, row in df.iterrows():\n",
        "            if pd.isna(df.at[idx, 'sampc_response']):  # Process only new rows or missing responses\n",
        "                sampc_summary = generate_sampc_summary(row['weeknotes'])\n",
        "                df.at[idx, 'sampc_response'] = sampc_summary\n",
        "\n",
        "            # Update the progress bar\n",
        "            pbar.update(1)\n",
        "\n",
        "            # Save progress every 10 iterations\n",
        "            if idx % 10 == 0:\n",
        "                df.to_pickle(fn_responses)\n",
        "                print(f\"Checkpoint saved at index {idx}, total cost so far: ${cb.total_cost:.4f}\")\n",
        "\n",
        "    # Save the final result\n",
        "    df.to_pickle(fn_responses)\n",
        "    print(\"Processing complete and final dataframe saved.\")\n",
        "    print(f\"Total cost: ${cb.total_cost:.4f}\")\n"
      ],
      "metadata": {
        "id": "8LzISH6fgctj",
        "outputId": "45fb1ee9-ab8f-43d0-ed1d-a2169e8f7a15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating SAMPC summaries:  18%|█▊        | 311/1758 [14:43<2:07:42,  5.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved at index 310, total cost so far: $0.0673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Creation, Splitting and Saving"
      ],
      "metadata": {
        "id": "Ow4eX46YVaQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert results to DataFrame and save\n",
        "df_sampc = pd.DataFrame([s.dict() for s in sampc_results])\n",
        "df_sampc['client_id'] = df['client_id']\n",
        "df_sampc['week'] = df['week']\n",
        "df_sampc['notes'] = df['notes']\n",
        "\n",
        "# Reorder columns\n",
        "df_sampc = df_sampc[['client_id', 'week', 'notes', 'somatiek', 'adl', 'mobiliteit', 'continentie', 'maatschappelijk', 'psychisch']]"
      ],
      "metadata": {
        "id": "RLgJnSGyb9Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Split the dataset and push to Hugging Face hub\n",
        "\n",
        "# # Convert df to Hugging Face dataset\n",
        "# dataset = Dataset.from_pandas(\n",
        "#     df=df_sampc,\n",
        "#     preserve_index=False\n",
        "# )\n",
        "\n",
        "# # Split the dataset into training(80%), validation(10%), and test(10%) sets\n",
        "# train_testvalid_split = dataset.train_test_split(\n",
        "#     test_size=0.2,\n",
        "#     seed=seed\n",
        "# )\n",
        "# test_valid_split = train_testvalid_split['test'].train_test_split(\n",
        "#     test_size=0.5,\n",
        "#     seed=seed\n",
        "# )\n",
        "\n",
        "# dataset_dict = DatasetDict({\n",
        "#     'train': train_testvalid_split['train'],\n",
        "#     'validation': test_valid_split['train'],\n",
        "#     'test': test_valid_split['test'],\n",
        "# })\n",
        "\n",
        "# # # Push the dataset to Hugging Face Hub\n",
        "# # dataset_dict.push_to_hub(path_hf_sampc,\n",
        "# #                          commit_message=commit_message,\n",
        "# #                          private=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "7nK8bqFamVC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U0k4NzR3dbp6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "gcai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}