{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ekrombouts/gcai_zuster_fietje/blob/main/notebooks/300_GenCareAISAMPCDatasetCreation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwx-li06oX8C"
   },
   "source": [
    "## Creating a SAMPC Dataset from GenCareAI Client records\n",
    "\n",
    "**Author:** Eva Rombouts  \n",
    "**Date:** 2024-09-16  \n",
    "**Updated:** 2024-10-17\n",
    "\n",
    "### Description\n",
    "This notebook summarizes nursing home client notes into the SAMPC format using OpenAI’s GPT model via LangChain. It processes the data, generates summaries, and prepares the dataset by splitting it into training, validation, and test sets, and uploads it to the Hugging Face Hub for use in machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIddGTJcE7Hf"
   },
   "source": [
    "## Environment Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # When in Colab\n",
    "# from google.colab import drive, userdata\n",
    "# import os\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "# base_dir = \"/content/drive/My Drive/Colab Notebooks/GenCareAI\"\n",
    "# open_ai_api_key = userdata.get(\"GCI_OPENAI_API_KEY\")\n",
    "\n",
    "# !pip install -q datasets langchain langchain_community langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When running locally\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path(os.getcwd()).resolve().parents[0]\n",
    "open_ai_api_key = os.getenv(\"GCI_OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OOrSISU7r15"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Progress bar\n",
    "from datasets import load_dataset, Dataset, DatasetDict  # For loading and managing datasets\n",
    "from typing import List\n",
    "\n",
    "# Langchain modules\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "\n",
    "# Pydantic library for data validation\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# OpenAI API integration using langchain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# # Scikit-learn library for splitting datasets into training and testing sets\n",
    "# from sklearn.model_selection import train_test_split\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQEKWDLNv1QJ"
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "seed = 6\n",
    "\n",
    "# Data paths\n",
    "nursing_care_home_name = \"Olympia\"\n",
    "# For reading data\n",
    "path_hf_records = f\"ekrombouts/{nursing_care_home_name}_records\"\n",
    "path_hf_clients = f\"ekrombouts/{nursing_care_home_name}_clients\"\n",
    "\n",
    "# For writing data\n",
    "path_hf_sampc = f\"ekrombouts/{nursing_care_home_name}_SAMPC_dataset\"\n",
    "commit_message = \"SAMPC dataset\"\n",
    "\n",
    "# File path for saving the generated SAMPC summaries\n",
    "fn_responses = os.path.join(base_dir, f\"data/care_pal/{nursing_care_home_name}_SAMPC_dataset.pkl\")\n",
    "\n",
    "# Settings for SAMPC summary generation\n",
    "model = \"gpt-4o-mini-2024-07-18\"\n",
    "temperature = 0.3\n",
    "\n",
    "sep_line = 50 * '-'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoCTuwkGJZov"
   },
   "source": [
    "## Loading and Preprocessing Data\n",
    "Client records and notes from fictional clients of a nursing home are loaded, cleaned, and processed. The notes are grouped by week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Vc3BWewlask"
   },
   "outputs": [],
   "source": [
    "# Load dataset from Hugging Face and preprocess\n",
    "dataset = load_dataset(path_hf_records)\n",
    "df_records = dataset['train'].to_pandas()\n",
    "\n",
    "# Floor datetime to the first day of the month\n",
    "df_records['week'] = df_records['datetime'].dt.to_period('W').dt.to_timestamp()\n",
    "\n",
    "# Group records by 'client_id' and 'month', concatenating notes into one string\n",
    "df = (df_records\n",
    "    .dropna()\n",
    "    .assign(week=lambda df: pd.to_datetime(df['datetime']).dt.to_period('W').dt.to_timestamp()) # Add 'week' column\n",
    "    .groupby(['client_id', 'week'])\n",
    "    .agg({'note': lambda x: '\\n'.join(x)}) # Concatenate 'note' values\n",
    "    .reset_index()\n",
    "    .rename(columns={'note': 'weeknotes'})\n",
    ")\n",
    "\n",
    "if verbose:\n",
    "  print(f\"Rows in original df: {df_records.shape[0]}, rows in processed df: {df.shape[0]}\\n\")\n",
    "  print(f\"SAMPLES{sep_line}\\n{df.sample(3)}\\n\")\n",
    "  print(f\"\\nContext column (weeknotes) example:{sep_line}\\n{df['weeknotes'].iloc[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMgbJRRcVB0x"
   },
   "source": [
    "## LLM Response Generation: SAMPC Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOAyIZuCU4By"
   },
   "outputs": [],
   "source": [
    "# Define the SAMPC model using Pydantic to structure the summarised data\n",
    "class SAMPC(BaseModel):\n",
    "    somatiek: List[str] = Field(description=\"lichamelijke klachten\")\n",
    "    adl: str = Field(description=\"beschrijf welke hulp de cliënt nodig heeft bij wassen en kleden\")\n",
    "    mobiliteit: str = Field(description=\"beschrijf de mobiliteit (bv rolstoelafhankelijk, gebruik rollator, valgevaar)\")\n",
    "    continentie: str = Field(description=\"continentie\")\n",
    "    maatschappelijk: str = Field(description=\"beschrijf bijzonderheden familie en dagbesteding\")\n",
    "    psychisch: List[str] = Field(description=\"beschrijf cognitie en probleemgedrag\")\n",
    "\n",
    "\n",
    "# Set up a parser to handle the output and inject instructions into the prompt template\n",
    "pyd_parser = PydanticOutputParser(pydantic_object=SAMPC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POwEfuzncjyx"
   },
   "outputs": [],
   "source": [
    "template=\"\"\"\n",
    "Vat de onderstaande rapportages kort en bondig samen om een profiel van de cliënt te schetsen met de volgende categorieën:\n",
    "\n",
    "Categorieën:\n",
    "- Somatiek\n",
    "- Wassen en aankleden\n",
    "- Mobiliteit\n",
    "- Continentie\n",
    "- Maatschappelijk\n",
    "- Psychisch\n",
    "\n",
    "Belangrijk:\n",
    "- Gebruik uitsluitend informatie uit de rapportages. Voeg geen eigen interpretaties toe.\n",
    "- Als er geen informatie beschikbaar is voor een categorie, noteer dan 'geen informatie beschikbaar' voor die categorie.\n",
    "- Richt je op algemene observaties en patronen, zonder de details van de rapportages over te nemen.\n",
    "\n",
    "---\n",
    "RAPPORTAGES:\n",
    "{rapportages}\n",
    "---\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"rapportages\"],\n",
    "    partial_variables={\"format_instructions\": pyd_parser.get_format_instructions()},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWTHsN8KYkIa"
   },
   "outputs": [],
   "source": [
    "# Initialize OpenAI Chat model\n",
    "llm = ChatOpenAI(\n",
    "    api_key=setup.get_openai_key(),\n",
    "    model=model,\n",
    "    temperature=temperature\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | pyd_parser\n",
    "\n",
    "\n",
    "if verbose:\n",
    "    sample_id = 50\n",
    "    sample_context = df['weeknotes'].iloc[sample_id]\n",
    "\n",
    "    sample_prompt = template.format(\n",
    "            rapportages=sample_context,\n",
    "            format_instructions=pyd_parser.get_format_instructions()\n",
    "    )\n",
    "\n",
    "    result = chain.invoke({\"rapportages\": sample_context})\n",
    "\n",
    "    print(sample_prompt)\n",
    "    print(\"RESPONSE\")\n",
    "    print(\"Somatiek:\\t\", result.somatiek)\n",
    "    print(\"ADL:\\t\\t\", result.adl)\n",
    "    print(\"Mobiliteit:\\t\", result.mobiliteit)\n",
    "    print(\"Continentie:\\t\", result.continentie)\n",
    "    print(\"Maatschappelijk:\", result.maatschappelijk)\n",
    "    print(\"Psychisch:\\t\", result.psychisch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LzISH6fgctj"
   },
   "outputs": [],
   "source": [
    "# Function to generate the SAMPC summary with error handling\n",
    "def generate_sampc_summary(notes: str) -> SAMPC:\n",
    "    try:\n",
    "        result = chain.invoke({\"rapportages\": notes})\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating SAMPC summary: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the previously saved dataframe if it exists, otherwise start fresh\n",
    "if os.path.exists(fn_responses):\n",
    "    df = pd.read_pickle(fn_responses)\n",
    "else:\n",
    "    df['sampc_response'] = None  # Ensure the column exists\n",
    "\n",
    "# Create a callback instance to track cost\n",
    "with get_openai_callback() as cb:\n",
    "\n",
    "    # Generate Summaries, process only new entries\n",
    "    with tqdm(total=len(df), desc=\"Generating SAMPC summaries\") as pbar:  # Set the total for the progress bar\n",
    "        for idx, row in df.iterrows():\n",
    "            if pd.isna(df.at[idx, 'sampc_response']):  # Process only new rows or missing responses\n",
    "                sampc_summary = generate_sampc_summary(row['weeknotes'])\n",
    "                df.at[idx, 'sampc_response'] = sampc_summary\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Save progress every 10 iterations\n",
    "            if idx % 100 == 0:\n",
    "                df.to_pickle(fn_responses)\n",
    "                print(f\"Checkpoint saved at index {idx}, total cost so far: ${cb.total_cost:.4f}\")\n",
    "\n",
    "    # Save the final result\n",
    "    df.to_pickle(fn_responses)\n",
    "    print(\"Processing complete and final dataframe saved.\")\n",
    "    print(f\"Total cost: ${cb.total_cost:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ow4eX46YVaQA"
   },
   "source": [
    "## Dataset Creation, Splitting and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DOEwcVy0VdnP"
   },
   "outputs": [],
   "source": [
    "# Method chaining to parse and flatten 'sampc_response' column\n",
    "df_parsed = (df['sampc_response']\n",
    "             .apply(lambda x: x.dict())\n",
    "             .pipe(pd.json_normalize))  # Use .pipe to chain json_normalize\n",
    "\n",
    "# Concatenate the parsed DataFrame to the original\n",
    "df_sampc_wide = pd.concat([df, df_parsed], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0k4NzR3dbp6"
   },
   "outputs": [],
   "source": [
    "# Drop the 'sampc_response' column and rename 'weeknotes' to 'context'\n",
    "df_sampc = df_sampc_wide.drop(columns=['sampc_response']).rename(columns={'weeknotes': 'context'})\n",
    "\n",
    "# Reshape the dataframe to long format with 'instruction' and 'response' columns\n",
    "df_sampc = df_sampc.melt(\n",
    "    id_vars=['client_id', 'week', 'context'],  # These columns remain the same\n",
    "    value_vars=['somatiek', 'adl', 'mobiliteit', 'continentie', 'maatschappelijk', 'psychisch'],  # Columns to reshape\n",
    "    var_name='instruction',  # New column for the former column names\n",
    "    value_name='response'  # New column for the values of the reshaped columns\n",
    ")\n",
    "\n",
    "# Map the 'instruction' values to the updated descriptions\n",
    "instruction_map = {\n",
    "    'somatiek': \"Beschrijf de lichamelijke klachten van de cliënt.\",\n",
    "    'adl': \"Beschrijf welke hulp de cliënt nodig heeft bij wassen en kleden.\",\n",
    "    'mobiliteit': \"Beschrijf de mobiliteit van de cliënt.\",\n",
    "    'continentie': \"Beschrijf de continentie van de cliënt.\",\n",
    "    'maatschappelijk': \"Beschrijf bijzonderheden rondom familie en dagbesteding van de cliënt.\",\n",
    "    'psychisch': \"Beschrijf de cognitie en gedragsproblemen van de cliënt.\"\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'instruction' column\n",
    "df_sampc['instruction'] = df_sampc['instruction'].map(instruction_map)\n",
    "\n",
    "# Convert lists in 'response' column to strings\n",
    "df_sampc['response'] = df_sampc['response'].apply(\n",
    "    lambda x: ', '.join(x) if isinstance(x, list) else x\n",
    ")\n",
    "\n",
    "if verbose:\n",
    "    # Check the first few rows of the transformed dataset\n",
    "    print(df_sampc.sample(2))\n",
    "    print(df_sampc.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nK8bqFamVC1"
   },
   "outputs": [],
   "source": [
    "# Split the dataset and push to Hugging Face hub\n",
    "\n",
    "# Convert df to Hugging Face dataset\n",
    "dataset = Dataset.from_pandas(\n",
    "    df=df_sampc,\n",
    "    preserve_index=False\n",
    ")\n",
    "\n",
    "# Split the dataset into training(80%), validation(10%), and test(10%) sets\n",
    "train_testvalid_split = dataset.train_test_split(\n",
    "    test_size=0.2,\n",
    "    seed=seed\n",
    ")\n",
    "test_valid_split = train_testvalid_split['test'].train_test_split(\n",
    "    test_size=0.5,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_testvalid_split['train'],\n",
    "    'validation': test_valid_split['train'],\n",
    "    'test': test_valid_split['test'],\n",
    "})\n",
    "\n",
    "# # Push the dataset to Hugging Face Hub\n",
    "# dataset_dict.push_to_hub(path_hf_sampc,\n",
    "#                          commit_message=commit_message,\n",
    "#                          private=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hW_DThatVF7D"
   },
   "outputs": [],
   "source": [
    "dataset_dict #['test'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtxfIgqIXzZx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gcai_zuster_fietje",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
