{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekrombouts/gcai_zuster_fietje/blob/main/notebooks/322_fietje_finetuning_instruct.ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFQItLAvRWVn"
      },
      "source": [
        "# Finetuning Fietje Instruct\n",
        "\n",
        "**Author:** Eva Rombouts  \n",
        "**Date:** 2024-10-14  \n",
        "\n",
        "### Description\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YG-3kw5SUqf-"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets\n",
        "\n",
        "verbose = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wuh3rwP3UtX2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import time\n",
        "from google.colab import runtime, drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The base model from Hugging Face that will be finetuned\n",
        "base_model = \"BramVanroy/fietje-2-instruct\"\n",
        "\n",
        "# The name of the finetuned model to be saved\n",
        "finetuned_model = \"zuster_fietje\"\n",
        "\n",
        "# Commit message for version control\n",
        "commit_message = \"Finetuned BramVanroy/fietje-2-instruct on ekrombouts/Gardenia_instruct_dataset\"\n",
        "\n",
        "# Path to the dataset on Hugging Face that will be used for finetuning\n",
        "path_dataset = \"ekrombouts/Gardenia_instruct_dataset\""
      ],
      "metadata": {
        "id": "UOHb32AZ9qMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18GH8sMHUtSs"
      },
      "outputs": [],
      "source": [
        "# Load the base model for text generation, automatically choosing the device (CPU or GPU)\n",
        "# and using bfloat16 precision to save memory.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "# Load the tokenizer associated with the base model\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "\n",
        "# Set the padding token to be the same as the end-of-sequence (EOS) token to avoid warnings\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "if verbose:\n",
        "    # Print the memory footprint of the loaded model in GB for monitoring\n",
        "    print(f\"Memory footprint: {model.get_memory_footprint() / 1e9} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhohK580UtM8"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "dataset = load_dataset(path_dataset)\n",
        "train_dataset = dataset['train']\n",
        "val_dataset = dataset['validation']\n",
        "\n",
        "if verbose:\n",
        "    print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt(row: dict, add_response: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Generates a prompt based on the input data in 'row'.\n",
        "\n",
        "    Args:\n",
        "        row (dict): A dictionary containing 'context', 'instruction', and optionally 'response'.\n",
        "        full (bool): If True, the prompt will include the 'response'.\n",
        "                     If False, only 'context' and 'instruction' will be included.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated prompt in text format.\n",
        "    \"\"\"\n",
        "    # Base prompt (without response)\n",
        "    prompt = f\"\"\"Context:\n",
        "{row['context']}\n",
        "\n",
        "Instructie:\n",
        "{row['instruction']}\n",
        "\n",
        "Antwoord:\n",
        "\"\"\"\n",
        "    # Append response if 'full' is True\n",
        "    if add_response:\n",
        "        prompt += f\"\\nAntwoord:\\n{row['response']}\\n\"\n",
        "\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "ucn8u_Fy-yQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a random example of the model's output before training\n",
        "if verbose:\n",
        "    import random\n",
        "    row = random.choice(train_dataset)  # Select a random row from the training dataset\n",
        "    prompt = create_prompt(row, False)  # Create the prompt from the selected dataset row\n",
        "    print(prompt)\n",
        "\n",
        "    # Convert the prompt into tokens that the model can understand\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "    attention_mask = tokenizer(prompt, return_tensors=\"pt\", padding=True).attention_mask.to(model.device)\n",
        "\n",
        "    # Enable the model's cache for faster generation and switch to evaluation mode\n",
        "    model.config.use_cache = True\n",
        "    model.eval()\n",
        "\n",
        "    # Generate a response based on the input prompt\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=150,  # Limit the number of new tokens generated to 150\n",
        "        do_sample=True,      # Use sampling to introduce randomness into the generation\n",
        "        top_p=0.95,          # Use nucleus sampling with a probability threshold of 0.95\n",
        "        top_k=50,            # Consider the top 50 tokens when sampling for each step\n",
        "        temperature=0.7,     # Set the temperature to 0.7 to control randomness (lower = more conservative)\n",
        "        num_return_sequences=1,  # Generate only one sequence\n",
        "        eos_token_id=tokenizer.eos_token_id,  # End the generation when the EOS token is reached\n",
        "        pad_token_id=tokenizer.eos_token_id   # Use the EOS token for padding\n",
        "    )\n",
        "\n",
        "    # Convert the generated token sequence back into text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    generated_response = generated_text[len(prompt):].strip()  # Remove the prompt part from the output\n",
        "\n",
        "    # Display the generated response and the actual reference response from the dataset\n",
        "    print(\"GENERATED RESPONSE:\")\n",
        "    print(generated_response)\n",
        "    print(\"\\nREFERENCE RESPONSE:\")\n",
        "    print(row['response'])"
      ],
      "metadata": {
        "id": "FpZ_HuadBdRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoXprhUXU5hL"
      },
      "outputs": [],
      "source": [
        "def collate_and_tokenize(row):\n",
        "    \"\"\"\n",
        "    Tokenizes and prepares a dataset sample for training.\n",
        "\n",
        "    Args:\n",
        "        row (dict): A single row or sample from the dataset, typically containing\n",
        "                    input text fields.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing tokenized input tensors and labels, with keys:\n",
        "              - 'input_ids': Tokenized input IDs for the model.\n",
        "              - 'attention_mask': Attention mask indicating which tokens should be attended to.\n",
        "              - 'labels': Tokenized labels for model training, identical to input_ids.\n",
        "    \"\"\"\n",
        "    prompt = create_prompt(row)  # Generate the prompt from the dataset row\n",
        "\n",
        "    # Tokenize the prompt and prepare input tensors\n",
        "    encoded = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",  # Return tensors in PyTorch format\n",
        "        padding=\"max_length\",  # Pad the input to the maximum length\n",
        "        truncation=True,       # Truncate inputs that are longer than the max length\n",
        "        max_length=2048,       # Set the maximum length for input tokens\n",
        "    )\n",
        "\n",
        "    # Create labels by duplicating input IDs for the model to predict\n",
        "    encoded[\"labels\"] = encoded[\"input_ids\"].clone()\n",
        "\n",
        "    return encoded  # Return the tokenized data with labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lwwu43-BVBrt"
      },
      "outputs": [],
      "source": [
        "# Columns that are not needed after tokenization and can be removed\n",
        "columns_to_remove = ['context', 'instruction', 'response']\n",
        "\n",
        "# Tokenize the training dataset and remove unnecessary columns\n",
        "tokenized_dataset_train = train_dataset.map(\n",
        "    collate_and_tokenize,  # Apply the tokenization function to each sample\n",
        "    batched=True,          # Process the dataset in batches\n",
        "    batch_size=1,          # Set batch size to 1 to process one sample at a time\n",
        "    remove_columns=columns_to_remove  # Remove columns that are no longer needed\n",
        ")\n",
        "\n",
        "# Tokenize the validation dataset and remove unnecessary columns\n",
        "tokenized_dataset_val = val_dataset.map(\n",
        "    collate_and_tokenize,\n",
        "    batched=True,\n",
        "    batch_size=1,\n",
        "    remove_columns=columns_to_remove\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLTDqcdXVB3V"
      },
      "outputs": [],
      "source": [
        "if verbose:\n",
        "    def print_trainable_parameters(model):\n",
        "        \"\"\"\n",
        "        Prints the number of trainable parameters in the model, along with the total number of parameters,\n",
        "        and the percentage of trainable parameters.\n",
        "\n",
        "        Args:\n",
        "            model (torch.nn.Module): The model to inspect.\n",
        "        \"\"\"\n",
        "        trainable_params = 0  # Counter for the number of trainable parameters\n",
        "        all_param = 0         # Counter for the total number of parameters\n",
        "\n",
        "        # Iterate over all model parameters\n",
        "        for _, param in model.named_parameters():\n",
        "            all_param += param.numel()  # Add the total number of parameters\n",
        "            if param.requires_grad:     # Check if the parameter is trainable\n",
        "                trainable_params += param.numel()  # Add the number of trainable parameters\n",
        "\n",
        "        # Print the results: number of trainable parameters, total parameters, and percentage of trainable parameters\n",
        "        print(\n",
        "            f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}%\"\n",
        "        )\n",
        "\n",
        "    # Call the function to print the trainable parameters of the model\n",
        "    print_trainable_parameters(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znY8COloVCCD"
      },
      "outputs": [],
      "source": [
        "# Enable gradient checkpointing to reduce memory usage during training by saving memory at the cost of some speed.\n",
        "# This allows the model to compute gradients on smaller chunks, which is useful for large models.\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Set the model to training mode, allowing layers like dropout and batchnorm to behave accordingly during training.\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZ6Gw9oKVMhe"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/results_full',  # Directory where the model checkpoints and outputs will be saved\n",
        "    report_to='none',  # Disable reporting to any platform (e.g., TensorBoard, WandB)\n",
        "    overwrite_output_dir=True,  # Overwrite the contents of the output directory if it already exists\n",
        "    num_train_epochs=1,  # Number of training epochs\n",
        "    per_device_train_batch_size=1,  # Batch size per device for training\n",
        "    per_device_eval_batch_size=1,  # Batch size per device for evaluation\n",
        "    gradient_accumulation_steps=8,  # Accumulate gradients over 8 steps before updating the model\n",
        "    gradient_checkpointing=True,  # Enable gradient checkpointing to reduce memory usage during backpropagation\n",
        "    warmup_steps=50,  # Number of warmup steps for the learning rate scheduler\n",
        "    logging_dir='/content/drive/MyDrive/logs',  # Directory for saving logs\n",
        "    logging_strategy=\"steps\",  # Log training information every few steps\n",
        "    logging_steps=50,  # Log every 50 steps\n",
        "    save_strategy=\"steps\",  # Save the model at regular step intervals\n",
        "    save_steps=100,  # Save the model every 100 steps\n",
        "    save_total_limit=2,  # Limit the number of saved checkpoints to the 2 most recent ones\n",
        "    evaluation_strategy=\"steps\",  # Evaluate the model at regular step intervals\n",
        "    eval_steps=100,  # Evaluate the model every 100 steps\n",
        "    load_best_model_at_end=True,  # Automatically load the best model when training is finished\n",
        "    bf16=True,  # Use bfloat16 precision for faster training with less memory usage\n",
        "    learning_rate=5e-5,  # Initial learning rate for the optimizer\n",
        "    weight_decay=0.01,  # Apply weight decay to the optimizer to avoid overfitting\n",
        "    resume_from_checkpoint=True  # Resume training from the last checkpoint, if available\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZnr_1tTVdLy"
      },
      "outputs": [],
      "source": [
        "# Disable caching during training for models that support caching, to save memory.\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Initialize the Trainer object with the model, datasets, training arguments, and early stopping.\n",
        "trainer = Trainer(\n",
        "    model=model,  # The model to be trained\n",
        "    train_dataset=tokenized_dataset_train,  # The tokenized training dataset\n",
        "    eval_dataset=tokenized_dataset_val,  # The tokenized validation dataset\n",
        "    args=training_args,  # Training arguments defined earlier\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # Early stopping after 3 evaluations without improvement\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10s_P5pjViMA"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()  # Record the start time before training begins\n",
        "trainer.train()  # Start the training process\n",
        "end_time = time.time()  # Record the end time after training finishes\n",
        "\n",
        "# Calculate and print the total training time in seconds\n",
        "training_time = end_time - start_time\n",
        "print(f\"Training completed in {training_time} seconds.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9axjL0KYVhwA"
      },
      "outputs": [],
      "source": [
        "# Push trained model and tokenizer to Hugging Face Hub\n",
        "model.push_to_hub(finetuned_model, use_auth_token=True, commit_message=commit_message, private=True)\n",
        "tokenizer.push_to_hub(finetuned_model, use_auth_token=True, commit_message=commit_message)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xG-12n6pUjdg"
      },
      "outputs": [],
      "source": [
        "# top Colab runtime (if applicable)\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpHFXAj4zJ4v"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "gcai_zf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}