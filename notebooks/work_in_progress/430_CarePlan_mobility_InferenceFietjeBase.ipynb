{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPhAoOZrkU0HH86oNu0UwOx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekrombouts/GenCareAI/blob/work_in_progress/scripts/work_in_progress/430_CarePlan_mobility_InferenceFietjeBase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers datasets\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n"
      ],
      "metadata": {
        "id": "vJixeMEjpkFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model and tokenizer\n",
        "model_name = \"ekrombouts/fietje_zorgplan_base\"  # Replace with the correct path if necessary\n",
        "model_tokenizer = \"BramVanroy/fietje-2\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map='auto',\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_tokenizer)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Ensure that model can use cache for inference\n",
        "model.config.use_cache = True\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "JLv_diSOp3r2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the validation dataset\n",
        "dataset = load_dataset(\"ekrombouts/Galaxy_SAMPC\")\n",
        "val_dataset = dataset['validation']\n",
        "\n",
        "# Define the tokenizer functions\n",
        "def truncate_notes_to_fit_prompt(notes, max_length=1800):\n",
        "    \"\"\"\n",
        "    Tokenize and truncate the 'notes' text to fit within the maximum length.\n",
        "    \"\"\"\n",
        "    # Tokenize and truncate the 'notes' to max_length\n",
        "    tokens = tokenizer(notes, return_tensors=\"np\", truncation=True, max_length=max_length)\n",
        "\n",
        "    # Decode the truncated tokens back to text\n",
        "    truncated_notes = tokenizer.decode(tokens[\"input_ids\"][0], skip_special_tokens=True)\n",
        "\n",
        "    return truncated_notes\n",
        "\n",
        "# Function to add the 'truncated_notes' column\n",
        "def add_truncated_notes(example):\n",
        "    notes_text = example[\"notes\"]\n",
        "    # Truncate the 'notes' to fit within the 1800 token limit\n",
        "    truncated_notes = truncate_notes_to_fit_prompt(notes_text, max_length=1800)\n",
        "    # Return the new field 'truncated_notes'\n",
        "    return {\"truncated_notes\": truncated_notes}\n",
        "\n",
        "# Add the 'truncated_notes' column to the val_dataset\n",
        "val_dataset = val_dataset.map(add_truncated_notes)"
      ],
      "metadata": {
        "id": "5JA6XG1HryFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the sample at index 3\n",
        "sample = val_dataset[1]\n",
        "\n",
        "# Prepare the prompt\n",
        "notes = sample['truncated_notes']\n",
        "mobiliteit_actual = sample['mobiliteit']\n",
        "\n",
        "prompt = f'''Lees de volgende rapportages en beschrijf de mobiliteit van de cliënt.\n",
        "\n",
        "Rapportages:\n",
        "{notes}\n",
        "\n",
        "Beschrijf de mobiliteit van de cliënt:\n",
        "'''\n",
        "\n",
        "# Tokenize the prompt\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "attention_mask = tokenizer(prompt, return_tensors=\"pt\", padding=True).attention_mask.to(model.device)\n"
      ],
      "metadata": {
        "id": "nbag58oSr16d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeiSyI59pZKj"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Genereer de output met de attention_mask\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    max_new_tokens=150,\n",
        "    do_sample=True,\n",
        "    top_p=0.95,\n",
        "    top_k=50,\n",
        "    temperature=0.7,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Decode the output\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Remove the prompt from the generated text to get only the model's output\n",
        "generated_response = generated_text[len(prompt):].strip()\n",
        "\n",
        "# Print the generated response and the actual 'mobiliteit' for comparison\n",
        "print(\"Generated Mobiliteit:\")\n",
        "print(generated_response)\n",
        "print(\"\\nActual Mobiliteit:\")\n",
        "print(mobiliteit_actual)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_response = generated_text\n",
        "print(full_response)\n"
      ],
      "metadata": {
        "id": "nHU-cY4Wr6iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JISMSkDQXpDA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}