{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNZKEhHu76Qm/8f+LGqKnKT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekrombouts/GenCareAI/blob/work_in_progress/scripts/work_in_progress/421_CarePlan_Psych_TrainFietjeBase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Fine-tuning a Dutch Causal Language Model for Nursing Home Note Summarization\n",
        "\n",
        "In this notebook, we fine-tune the Dutch Causal Language Model \"BramVanroy/fietje-2\" on a dataset of general nursing home notes\n",
        "to automatically summarize key points regarding cognitive and behavioral issues from the notes.\n",
        "The aim is to create a model that can read Dutch nursing home reports and generate summaries of a client's mental and behavioral state.\n",
        "\n",
        "### Key Steps:\n",
        "1. Load and preprocess the SAMPC dataset.\n",
        "2. Truncate care notes to fit within the model's input limits.\n",
        "3. Fine-tune the \"fietje-2\" model using PyTorch and Hugging Face's Trainer.\n",
        "4. Generate outputs and compare with actual psychological assessments.\n",
        "5. Push the fine-tuned model and tokenizer to the Hugging Face Hub.\n",
        "\n",
        "### Dataset:\n",
        "We use the \"ekrombouts/Galaxy_SAMPC\" dataset, which contains synthetic nursing care notes along with descriptions or summaries for the SAMPC categories:\n",
        "somatic, ADL (Activities of Daily Living), social, psychological, and communication aspects.\n",
        "\n",
        "### Expected Outcome:\n",
        "After fine-tuning, the model will generate summaries of cognitive and behavioral aspects from nursing home notes.\n",
        "While the output may not be fully reliable, this project serves to illustrate the potential of using language models for summarizing care data.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gQhDWtPbH01u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install -q transformers datasets\n"
      ],
      "metadata": {
        "id": "OlYZxF4MFQDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import time\n"
      ],
      "metadata": {
        "id": "j1bYGCpuFQ7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model and tokenizer\n",
        "path_hf_sampc = \"ekrombouts/Galaxy_SAMPC\"\n",
        "model_name = \"BramVanroy/fietje-2\"\n",
        "model_finetuned = \"fietje_zorgplan_psyche\"\n",
        "commit_message = \"Trained base model\"\n",
        "\n",
        "# Load the model with specified dtype and device map\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
        "\n",
        "# Print memory footprint of the model\n",
        "print(f\"Memory footprint: {model.get_memory_footprint() / 1e9} GB\")\n"
      ],
      "metadata": {
        "id": "t5u5_JNOFWXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "dataset = load_dataset(path_hf_sampc)\n",
        "train_dataset = dataset['train']\n",
        "val_dataset = dataset['validation']\n"
      ],
      "metadata": {
        "id": "vHZEJ42GFd9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define truncation function\n",
        "def truncate_notes_to_fit_prompt(notes, max_length=1800):\n",
        "    tokens = tokenizer(notes, return_tensors=\"np\", truncation=True, max_length=max_length)\n",
        "    truncated_notes = tokenizer.decode(tokens[\"input_ids\"][0], skip_special_tokens=True)\n",
        "    return truncated_notes\n",
        "\n",
        "# Add truncated notes to dataset\n",
        "def add_truncated_notes(example):\n",
        "    notes_text = example[\"notes\"]\n",
        "    truncated_notes = truncate_notes_to_fit_prompt(notes_text, max_length=1800)\n",
        "    return {\"truncated_notes\": truncated_notes}\n",
        "\n",
        "# Apply the truncation function to train and validation sets\n",
        "train_dataset = train_dataset.map(add_truncated_notes)\n",
        "val_dataset = val_dataset.map(add_truncated_notes)"
      ],
      "metadata": {
        "id": "wDcEzhF9Fu6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample generation with prompts and evaluation mode\n",
        "sample = val_dataset[1]\n",
        "notes = sample['truncated_notes']\n",
        "gedrag_actual = sample['psychisch']\n",
        "\n",
        "prompt = f'''Lees de volgende rapportages en beschrijf de cognitie en gedragsproblemen van de cliënt.\n",
        "\n",
        "Rapportages:\n",
        "{notes}\n",
        "\n",
        "Geef de output als lijst van strings, voorbeeld: [\"aap\", \"noot\", \"mies\"]\n",
        "\n",
        "Beschrijf cognitie/gedragsproblemen:\n",
        "'''\n",
        "\n",
        "# Tokenize the prompt\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "attention_mask = tokenizer(prompt, return_tensors=\"pt\", padding=True).attention_mask.to(model.device)\n",
        "\n",
        "# Enable cache and set model to evaluation mode\n",
        "model.config.use_cache = True\n",
        "model.eval()\n",
        "\n",
        "# Generate output\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    max_new_tokens=150,\n",
        "    do_sample=True,\n",
        "    top_p=0.95,\n",
        "    top_k=50,\n",
        "    temperature=0.7,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Decode generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "generated_response = generated_text[len(prompt):].strip()\n",
        "\n",
        "# Display generated response and actual response\n",
        "print(\"Generated response:\")\n",
        "print(generated_response)\n",
        "print(\"\\nActual:\")\n",
        "print(gedrag_actual)"
      ],
      "metadata": {
        "id": "dvskBQ_LFz6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to tokenize dataset samples\n",
        "def collate_and_tokenize(examples):\n",
        "    notes = examples[\"truncated_notes\"][0]\n",
        "    psychisch = examples[\"psychisch\"][0]\n",
        "\n",
        "    prompt = f'''Lees de volgende rapportages en beschrijf de cognitie en gedragsproblemen van de cliënt.\n",
        "\n",
        "Rapportages:\n",
        "{notes}\n",
        "\n",
        "Geef de output als lijst van strings, voorbeeld: [\"aap\", \"noot\", \"mies\"]\n",
        "\n",
        "Beschrijf cognitie/gedragsproblemen:\n",
        "{psychisch}\n",
        "'''\n",
        "\n",
        "    # Tokenize and create labels\n",
        "    encoded = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=2048,\n",
        "    )\n",
        "    encoded[\"labels\"] = encoded[\"input_ids\"].clone()\n",
        "    return encoded"
      ],
      "metadata": {
        "id": "iL3SgbEnGAMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the dataset and remove unnecessary columns\n",
        "columns_to_remove = ['ct_id', 'week', 'notes', 'somatiek', 'adl', 'mobiliteit', 'continentie', 'maatschappelijk', 'psychisch', 'truncated_notes']\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_dataset_train = train_dataset.map(collate_and_tokenize, batched=True, batch_size=1, remove_columns=columns_to_remove)\n",
        "tokenized_dataset_val = val_dataset.map(collate_and_tokenize, batched=True, batch_size=1, remove_columns=columns_to_remove)\n"
      ],
      "metadata": {
        "id": "hyNjJfqyGIhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print trainable parameters in the model\n",
        "def print_trainable_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}%\"\n",
        "    )\n",
        "\n",
        "print_trainable_parameters(model)\n"
      ],
      "metadata": {
        "id": "pyZGUySPGMt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable gradient checkpointing and set model to training mode\n",
        "model.gradient_checkpointing_enable()\n",
        "model.train()\n"
      ],
      "metadata": {
        "id": "JxkfvQvEGQ90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_full',\n",
        "    report_to='none',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=20,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    gradient_checkpointing=True,\n",
        "    warmup_steps=50,\n",
        "    logging_dir='./logs',\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    bf16=True,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        ")\n"
      ],
      "metadata": {
        "id": "qVNc_DOiGV-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_dataset_train,\n",
        "    eval_dataset=tokenized_dataset_val,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# Disable cache for training\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "1vZzsmkNGY31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model and measure training time\n",
        "start_time = time.time()  # Start time\n",
        "trainer.train()  # Start training\n",
        "end_time = time.time()  # End time\n",
        "\n",
        "training_time = end_time - start_time  # Total training time\n",
        "print(f\"Training completed in {training_time} seconds.\")"
      ],
      "metadata": {
        "id": "TdReitUBGgOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push trained model and tokenizer to Hugging Face Hub\n",
        "model.push_to_hub(model_finetuned, use_auth_token=True, commit_message=commit_message, private=True)\n",
        "tokenizer.push_to_hub(model_finetuned, use_auth_token=True, commit_message=commit_message)"
      ],
      "metadata": {
        "id": "dRd-xo1lGpcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop Colab runtime (if applicable)\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "sfa08h8tGq6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QMW-Yi8EG7jG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}