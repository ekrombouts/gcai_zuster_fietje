{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekrombouts/GenCareAI/blob/work_in_progress/scripts/work_in_progress/422_sampc_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets"
      ],
      "metadata": {
        "id": "EToO0FCbOPcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "FAAPAAhDOPRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained fine-tuned model and tokenizer\n",
        "model_finetuned = \"ekrombouts/gcai_sampc_fietje\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_finetuned, torch_dtype=torch.bfloat16, device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_finetuned)\n",
        "\n",
        "# Set pad token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n"
      ],
      "metadata": {
        "id": "orbb3oKWOPLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "path_hf_sampc = \"ekrombouts/Galaxy_SAMPC_long\"\n",
        "dataset = load_dataset(path_hf_sampc)\n",
        "# We will be working with the validation dataset\n",
        "val_dataset = dataset['validation']"
      ],
      "metadata": {
        "id": "5WHFpzUIOPGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable cache and set model to evaluation mode\n",
        "model.config.use_cache = True\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "dzOshXvHOo8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt(text, category=None):\n",
        "    if category == \"somatiek\":\n",
        "        prompt = f\"\"\"Lees de volgende rapportages en beschrijf de lichamelijke klachten van de cliënt.\n",
        "\n",
        "Rapportages:\n",
        "{text}\n",
        "\n",
        "Geef de output als lijst van strings: ['foo', 'bar'].\n",
        "\n",
        "Beschrijf de lichamelijke klachten:\n",
        "\"\"\"\n",
        "    elif category == \"adl\":\n",
        "        prompt = f\"\"\"Lees de volgende rapportages en beschrijf welke hulp de cliënt nodig heeft bij wassen en kleden.\n",
        "\n",
        "Rapportages:\n",
        "{text}\n",
        "\n",
        "Beschrijf de hulp bij wassen en kleden:\n",
        "\"\"\"\n",
        "    elif category == \"continentie\":\n",
        "        prompt = f\"\"\"Lees de volgende rapportages en beschrijf de continentie van de cliënt.\n",
        "\n",
        "Rapportages:\n",
        "{text}\n",
        "\n",
        "Beschrijf de continentie van de cliënt:\n",
        "\"\"\"\n",
        "    elif category == \"mobiliteit\":\n",
        "        prompt = f\"\"\"Lees de volgende rapportages en beschrijf de mobiliteit van de cliënt.\n",
        "\n",
        "Rapportages:\n",
        "{text}\n",
        "\n",
        "Beschrijf de mobiliteit van de cliënt:\n",
        "\"\"\"\n",
        "    elif category == \"maatschappelijk\":\n",
        "        prompt = f\"\"\"Lees de volgende rapportages en beschrijf de bijzonderheden rondom familie en dagbesteding van de cliënt.\n",
        "\n",
        "Rapportages:\n",
        "{text}\n",
        "\n",
        "Beschrijf de familie en dagbesteding:\n",
        "\"\"\"\n",
        "    elif category == \"psychisch\":\n",
        "        prompt = f\"\"\"Lees de volgende rapportages en beschrijf de cognitie en gedragsproblemen van de cliënt.\n",
        "\n",
        "Rapportages:\n",
        "{text}\n",
        "\n",
        "Geef de output als lijst van strings: ['foo', 'bar'].\n",
        "\n",
        "Beschrijf cognitie en gedragsproblemen:\n",
        "\"\"\"\n",
        "    else:\n",
        "        prompt = text\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "L5XYyn0tQpRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_prompt(prompt):\n",
        "    # Tokenize and prepare input\n",
        "    return tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device), \\\n",
        "           tokenizer(prompt, return_tensors=\"pt\", padding=True).attention_mask.to(model.device)\n",
        "\n",
        "def generate_output(input_ids, attention_mask):\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=150,\n",
        "            do_sample=True,\n",
        "            top_p=0.95,\n",
        "            top_k=50,\n",
        "            temperature=0.8,\n",
        "            num_return_sequences=1,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "def answer(notes, category=None):\n",
        "    prompt = make_prompt(notes, category)\n",
        "    input_ids, attention_mask = tokenize_prompt(prompt)\n",
        "    generated_text = generate_output(input_ids, attention_mask)\n",
        "    return generated_text[len(prompt):].strip()"
      ],
      "metadata": {
        "id": "0uVHKvxmnExH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the prompt with notes from sample\n",
        "sample = val_dataset[13]\n",
        "prompt = sample['prompt']\n",
        "print(prompt)\n",
        "\n",
        "# Display the generated response and actual response\n",
        "ref_response = sample['reference']  # Reference response from dataset\n",
        "print(\"GENERATED RESPONSE:\")\n",
        "print(answer(prompt))\n",
        "print(\"\\nREFERENCE RESPONSE:\")\n",
        "print(ref_response)"
      ],
      "metadata": {
        "id": "zxgAZuTgOpHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rapportages = \"\"\"U was inc van urine. U was niet vriendelijk tijdens het verschonen.\n",
        "Mw was vanmorgen incontinent van dunne def, bed was ook nat. Mw is volledig verzorgd, bed is verschoond,\n",
        "Mw. haar kledingkast is opgeruimd.\n",
        "Mw. zei:\"oooh kind, ik heb zo'n pijn. Mijn benen. Dat gaat nooit meer weg.\" Mw. zat in haar rolstoel en haar gezicht trok weg van de pijn en kreeg traanogen. Mw. werkte goed mee tijdens adl. en was vriendelijk aanwezig. Pijn. Mw. kreeg haar medicatie in de ochtend, waaronder pijnstillers. 1 uur later adl. gegeven.\n",
        "Ik lig hier maar voor Piet Snot. Mw. was klaarwakker tijdens eerste controle. Ze wilde iets, maar wist niet wat. Mw. een slokje water gegeven en uitgelegd hoe ze kon bellen als ze iets wilde. Mw. pakte mijn hand en bedankte me.\n",
        "Mevr. in de ochtend ondersteund met wassen en aankleden. Mevr was rustig aanwezig.\n",
        "Mw is volledig geholpen met ochtendzorg, mw haar haren zijn gewassen. Mw haar nagels zijn kort geknipt.\n",
        "Mevr heeft het ontbijt op bed genuttigd. Daarna mocht ik na de tweede poging Mevr ondersteunen met wassen en aankleden.\n",
        "Vanmorgen met mw naar buiten geweest om een sigaret te roken. Mw was niet erg spraakzaam en mw kwam op mij over alsof ze geen behoefte had aan een gesprek. Mw kreeg het koud door de wind en wilde snel weer naar binnen.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "E5qUhe8Tf1ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer(rapportages, \"somatiek\")"
      ],
      "metadata": {
        "id": "ZXXGRqS3lvuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer(rapportages, \"adl\")"
      ],
      "metadata": {
        "id": "lBzyQcnypX75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer(rapportages, \"continentie\")"
      ],
      "metadata": {
        "id": "dIUNa9lkpbp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer(rapportages, \"mobiliteit\")"
      ],
      "metadata": {
        "id": "bZ-6HwHhpch0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer(rapportages, \"maatschappelijk\")"
      ],
      "metadata": {
        "id": "qSl59DxipeUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer(rapportages, \"psychisch\")"
      ],
      "metadata": {
        "id": "ZmV10sy5pfpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"Lees de volgende rapportages.\n",
        "\n",
        "Rapportages:\n",
        "{rapportages}\n",
        "\n",
        "Geef een korte samenvatting:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "PZy6RFV4qfC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer(prompt)"
      ],
      "metadata": {
        "id": "9Muc3r3da2uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the prompt with truncated notes from sample\n",
        "sample = val_dataset[13]\n",
        "prompt = f\"\"\"Lees de volgende rapportages:\n",
        "\n",
        "Rapportages:\n",
        "{sample['notes']}\n",
        "\n",
        "Beschrijf het eten en drinken:\n",
        "\"\"\"\n",
        "print(prompt)\n",
        "print(100*\"-\")\n",
        "answer(prompt)"
      ],
      "metadata": {
        "id": "njvKGCkwa8P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer(sample['notes'], \"somatiek\")"
      ],
      "metadata": {
        "id": "gITvEYLLbo5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M4SjVnY0dJnh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}