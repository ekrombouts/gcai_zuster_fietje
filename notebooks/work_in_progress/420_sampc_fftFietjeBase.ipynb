{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNKCjl0GF/ODuhbIDUPJRJC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekrombouts/GenCareAI/blob/work_in_progress/scripts/work_in_progress/420_sampc_fftFietjeBase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning a Dutch Causal Language Model for Nursing Home Note Summarization\n",
        "\n",
        "In this notebook, we fine-tune the Dutch Causal Language Model \"BramVanroy/fietje-2\" on a dataset of general nursing home notes\n",
        "to automatically summarize key points regarding cognitive and behavioral issues from the notes.\n",
        "The aim is to create a model that can read Dutch nursing home reports and generate summaries of a client's mental and behavioral state.\n",
        "\n",
        "### Key Steps:\n",
        "1. Load the SAMPC dataset.\n",
        "2. Truncate care notes to fit within the model's input limits.\n",
        "3. Fine-tune the \"fietje-2\" model using PyTorch and Hugging Face's Trainer.\n",
        "4. Generate outputs and compare with actual psychological assessments.\n",
        "5. Push the fine-tuned model and tokenizer to the Hugging Face Hub.\n",
        "\n",
        "### Dataset:\n",
        "We use the \"ekrombouts/Galaxy_SAMPC\" dataset, which contains synthetic nursing care notes along with descriptions or summaries for the SAMPC categories:\n",
        "somatic, ADL (Activities of Daily Living), social, psychological, and communication aspects.\n",
        "\n",
        "### Expected Outcome:\n",
        "After fine-tuning, the model will generate summaries of cognitive and behavioral aspects from nursing home notes.\n",
        "While the output may not be fully reliable, this project serves to illustrate the potential of using language models for summarizing care data.\n"
      ],
      "metadata": {
        "id": "sFQItLAvRWVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install necessary libraries\n",
        "!pip install -q transformers datasets"
      ],
      "metadata": {
        "id": "YG-3kw5SUqf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Import required libraries and mount Google Drive\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import time\n",
        "from google.colab import runtime, drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Wuh3rwP3UtX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load pre-trained model and tokenizer\n",
        "path_hf_sampc = \"ekrombouts/Galaxy_SAMPC_long\"\n",
        "model_name = \"BramVanroy/fietje-2\"\n",
        "model_finetuned = \"gcai_sampc_fietje\"\n",
        "commit_message = \"Finetuned BramVanroy/fietje-2 on Galaxy_SAMPC_long\"\n",
        "\n",
        "# Load the model. Using bfloat16 gives lower precision but saves memory\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
        "\n",
        "# Print memory footprint of the model\n",
        "print(f\"Memory footprint: {model.get_memory_footprint() / 1e9} GB\")\n"
      ],
      "metadata": {
        "id": "18GH8sMHUtSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Load dataset\n",
        "dataset = load_dataset(path_hf_sampc)\n",
        "train_dataset = dataset['train']\n",
        "val_dataset = dataset['validation']"
      ],
      "metadata": {
        "id": "MhohK580UtM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "y_Z78IY6OBzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Show the results of the untrained model\n",
        "\n",
        "sample = val_dataset[6]\n",
        "prompt = sample['prompt']\n",
        "\n",
        "# Tokenize the prompt\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "attention_mask = tokenizer(prompt, return_tensors=\"pt\", padding=True).attention_mask.to(model.device)\n",
        "\n",
        "# Enable cache and set model to evaluation mode\n",
        "model.config.use_cache = True\n",
        "model.eval()\n",
        "\n",
        "# Generate output\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    max_new_tokens=150,\n",
        "    do_sample=True,\n",
        "    top_p=0.95,\n",
        "    top_k=50,\n",
        "    temperature=0.7,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Decode generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "generated_response = generated_text[len(prompt):].strip()\n",
        "\n",
        "# Display generated response and actual response\n",
        "print(\"GENERATED RESPONSE:\")\n",
        "print(generated_response)\n",
        "print(\"\\nREFERENCE RESPONSE:\")\n",
        "print(sample['reference'])"
      ],
      "metadata": {
        "id": "bhzNpSuyU54u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Function to tokenize dataset samples\n",
        "def collate_and_tokenize(examples):\n",
        "    prompt = examples[\"prompt\"][0]+examples[\"reference\"][0]\n",
        "\n",
        "    # Tokenize and create labels\n",
        "    encoded = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=2048,\n",
        "    )\n",
        "    encoded[\"labels\"] = encoded[\"input_ids\"].clone()\n",
        "    return encoded\n"
      ],
      "metadata": {
        "id": "VoXprhUXU5hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Tokenize the dataset and remove unnecessary columns\n",
        "columns_to_remove = ['notes', 'prompt', 'reference', 'category']\n",
        "# Apply tokenization\n",
        "tokenized_dataset_train = train_dataset.map(collate_and_tokenize, batched=True, batch_size=1, remove_columns=columns_to_remove)\n",
        "tokenized_dataset_val = val_dataset.map(collate_and_tokenize, batched=True, batch_size=1, remove_columns=columns_to_remove)\n"
      ],
      "metadata": {
        "id": "Lwwu43-BVBrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Print trainable parameters in the model\n",
        "def print_trainable_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}%\"\n",
        "    )\n",
        "\n",
        "print_trainable_parameters(model)\n"
      ],
      "metadata": {
        "id": "KLTDqcdXVB3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Enable gradient checkpointing and set model to training mode\n",
        "model.gradient_checkpointing_enable()\n",
        "model.train()"
      ],
      "metadata": {
        "id": "znY8COloVCCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/results_full',\n",
        "    report_to='none',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    gradient_checkpointing=True,\n",
        "    warmup_steps=50,\n",
        "    logging_dir='/content/drive/MyDrive/logs',\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    bf16=True,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    resume_from_checkpoint=True\n",
        ")"
      ],
      "metadata": {
        "id": "QZ6Gw9oKVMhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Initialize Trainer with EarlyStoppingCallback and disable cache for training\n",
        "\n",
        "# Disable cache for training\n",
        "model.config.use_cache = False\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_dataset_train,\n",
        "    eval_dataset=tokenized_dataset_val,\n",
        "    args=training_args,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # Add early stopping callback\n",
        ")"
      ],
      "metadata": {
        "id": "WZnr_1tTVdLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Train the model and measure training time\n",
        "start_time = time.time()  # Start time\n",
        "trainer.train()  # Start training\n",
        "end_time = time.time()  # End time\n",
        "\n",
        "training_time = end_time - start_time  # Total training time\n",
        "print(f\"Training completed in {training_time} seconds.\")\n"
      ],
      "metadata": {
        "id": "10s_P5pjViMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: Push trained model and tokenizer to Hugging Face Hub\n",
        "model.push_to_hub(model_finetuned, use_auth_token=True, commit_message=commit_message, private=True)\n",
        "tokenizer.push_to_hub(model_finetuned, use_auth_token=True, commit_message=commit_message)\n"
      ],
      "metadata": {
        "id": "9axjL0KYVhwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 15: Stop Colab runtime (if applicable)\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "xG-12n6pUjdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SpHFXAj4zJ4v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}