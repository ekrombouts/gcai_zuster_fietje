{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekrombouts/gcai_zuster_fietje/blob/main/notebooks/330_VraagHetZusterFietje.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bitsandbytes transformers peft datasets"
      ],
      "metadata": {
        "id": "EToO0FCbOPcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
        "from peft import PeftModel\n",
        "from datasets import load_dataset\n",
        "import random\n"
      ],
      "metadata": {
        "id": "FAAPAAhDOPRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained fine-tuned model and tokenizer\n",
        "instruct_model_id = \"ekrombouts/zuster_fietje\"\n",
        "peft_model_id = \"ekrombouts/zuster_fietje_peft\"\n",
        "\n",
        "# # Quantization configuration for inference\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_quant_type='nf4',\n",
        "#     bnb_4bit_compute_dtype='float16',\n",
        "#     bnb_4bit_use_double_quant=True\n",
        "# )\n",
        "\n",
        "# Load the model with PEFT configuration\n",
        "instruct_model = AutoModelForCausalLM.from_pretrained(\n",
        "    instruct_model_id,\n",
        "    device_map=\"auto\",\n",
        "    # quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Apply PEFT finetuning configuration to the model\n",
        "peft_model = PeftModel.from_pretrained(\n",
        "    instruct_model,\n",
        "    peft_model_id,\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    peft_model_id,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.truncation_side = 'left'\n"
      ],
      "metadata": {
        "id": "orbb3oKWOPLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "path_hf_sampc = \"ekrombouts/Gardenia_instruct_dataset\"\n",
        "dataset = load_dataset(path_hf_sampc)\n",
        "# We will be working with the validation dataset\n",
        "val_dataset = dataset['validation']"
      ],
      "metadata": {
        "id": "5WHFpzUIOPGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable cache and set model to evaluation mode\n",
        "peft_model.config.use_cache = True\n",
        "peft_model.eval()\n"
      ],
      "metadata": {
        "id": "dzOshXvHOo8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt(row: dict, add_response: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Generates a prompt based on the input data in 'row'.\n",
        "\n",
        "    Args:\n",
        "        row (dict): A dictionary containing 'context', 'instruction', and optionally 'response'.\n",
        "        full (bool): If True, the prompt will include the 'response'.\n",
        "                     If False, only 'context' and 'instruction' will be included.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated prompt in text format.\n",
        "    \"\"\"\n",
        "    # Base prompt (without response)\n",
        "    prompt = f\"\"\"Context:\n",
        "{row['context']}\n",
        "\n",
        "Instructie:\n",
        "{row['instruction']}\n",
        "Maak uitsluitend gebruik van de context en instructie om een antwoord te geven.\n",
        "\n",
        "Antwoord:\n",
        "\"\"\"\n",
        "    # Append response if 'add_response' is True\n",
        "    if add_response:\n",
        "        prompt += f\"\\nAntwoord:\\n{row['response']}\\n\"\n",
        "\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "L5XYyn0tQpRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_prompt(model, prompt):\n",
        "    # Tokenize and prepare input\n",
        "    return tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device), \\\n",
        "           tokenizer(prompt, return_tensors=\"pt\", padding=True).attention_mask.to(model.device)\n",
        "\n",
        "def generate_output(model, input_ids, attention_mask):\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=150,\n",
        "            do_sample=True,\n",
        "            top_p=0.95,\n",
        "            top_k=50,\n",
        "            temperature=0.8,\n",
        "            num_return_sequences=1,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "def answer(model, prompt):\n",
        "    input_ids, attention_mask = tokenize_prompt(model, prompt)\n",
        "    generated_text = generate_output(\n",
        "        model=model,\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask)\n",
        "    return generated_text[len(prompt):].strip()"
      ],
      "metadata": {
        "id": "0uVHKvxmnExH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the prompt with notes from sample\n",
        "# row = val_dataset[2]\n",
        "row = random.choice(val_dataset)  # Select a random row from the training dataset\n",
        "\n",
        "prompt = create_prompt(\n",
        "    row=row,\n",
        "    add_response=False\n",
        ")\n",
        "print(prompt)\n",
        "\n",
        "# Display the generated response and actual response\n",
        "ref_response = row['response']  # Reference response from dataset\n",
        "print(\"\\nREFERENCE RESPONSE:\")\n",
        "print(ref_response)\n",
        "print(f\"\\n{100*'-'}\")\n",
        "print(\"ZUSTER FIETJE:\")\n",
        "print(answer(instruct_model,prompt))\n",
        "print(f\"\\n{100*'-'}\")\n",
        "print(\"PEFT MODEL:\")\n",
        "print(answer(peft_model,prompt))"
      ],
      "metadata": {
        "id": "zxgAZuTgOpHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Rapportages:\n",
        "\n",
        "U was inc van urine. U was niet vriendelijk tijdens het verschonen.\n",
        "Mw was vanmorgen incontinent van dunne def, bed was ook nat. Mw is volledig verzorgd, bed is verschoond,\n",
        "Mw. haar kledingkast is opgeruimd.\n",
        "Mw. zei:\"oooh kind, ik heb zo'n pijn. Mijn benen. Dat gaat nooit meer weg.\" Mw. zat in haar rolstoel en haar gezicht trok weg van de pijn en kreeg traanogen. Mw. werkte goed mee tijdens adl. en was vriendelijk aanwezig. Pijn. Mw. kreeg haar medicatie in de ochtend, waaronder pijnstillers. 1 uur later adl. gegeven.\n",
        "Ik lig hier maar voor Piet Snot. Mw. was klaarwakker tijdens eerste controle. Ze wilde iets, maar wist niet wat. Mw. een slokje water gegeven en uitgelegd hoe ze kon bellen als ze iets wilde. Mw. pakte mijn hand en bedankte me.\n",
        "Mevr. in de ochtend ondersteund met wassen en aankleden. Mevr was rustig aanwezig.\n",
        "Mw is volledig geholpen met ochtendzorg, mw haar haren zijn gewassen. Mw haar nagels zijn kort geknipt.\n",
        "Mevr heeft het ontbijt op bed genuttigd. Daarna mocht ik na de tweede poging Mevr ondersteunen met wassen en aankleden.\n",
        "Vanmorgen met mw naar buiten geweest om een sigaret te roken. Mw was niet erg spraakzaam en mw kwam op mij over alsof ze geen behoefte had aan een gesprek. Mw kreeg het koud door de wind en wilde snel weer naar binnen.\n",
        "\n",
        "Instructie:\n",
        "Heeft mw hulp nodig bij eten en drinken?\n",
        "\n",
        "Antwoord:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "E5qUhe8Tf1ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ZUSTER FIETJE:\")\n",
        "print(answer(instruct_model,prompt))\n",
        "print(f\"\\n{100*'-'}\")\n",
        "print(\"PEFT MODEL:\")\n",
        "print(answer(model,prompt))"
      ],
      "metadata": {
        "id": "M4SjVnY0dJnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mZtU77bylO_E"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}