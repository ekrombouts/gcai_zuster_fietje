{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ekrombouts/gcai_zuster_fietje/blob/main/notebooks/300_GenCareAIQADatasetCreation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwx-li06oX8C"
   },
   "source": [
    "### Creating a QA Dataset from GenCareAI Client Records\n",
    "\n",
    "**Author:** Eva Rombouts  \n",
    "**Date:** 2024-10-16  \n",
    "\n",
    "### Description\n",
    "This notebook creates a dataset from client records and predefined instruction prompts. Healthcare context notes are matched with relevant instructions using embeddings and cosine similarity. A language model generates responses to these instructions, and the final dataset is split into training, validation, and test sets. The dataset is then saved locally and can be uploaded to Hugging Face for further use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DY2bjoMMby3l"
   },
   "source": [
    "## Environment Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When in Colab\n",
    "from google.colab import drive, userdata\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "base_dir = \"/content/drive/My Drive/Colab Notebooks/GenCareAI\"\n",
    "open_ai_api_key = userdata.get(\"GCI_OPENAI_API_KEY\")\n",
    "\n",
    "!pip install -q datasets sentence-transformers langchain langchain_openai langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # When running locally\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "\n",
    "# base_dir = Path(os.getcwd()).resolve().parents[0]\n",
    "# open_ai_api_key = os.getenv(\"GCI_OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OOrSISU7r15"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# For data splitting and similarity calculations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Importing libraries for working with LLM prompts and OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "\n",
    "# Torch for deep learning and sentence transformers for embeddings\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Progress bar utilities\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "\n",
    "# Hugging Face dataset utilities\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQEKWDLNv1QJ"
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "seed = 6\n",
    "\n",
    "# Data paths\n",
    "nursing_care_home_name = \"Gardenia\"\n",
    "# For reading data\n",
    "path_hf_records = f\"ekrombouts/{nursing_care_home_name}_records\"\n",
    "path_hf_clients = f\"ekrombouts/{nursing_care_home_name}_clients\"\n",
    "\n",
    "# For writing data\n",
    "path_hf_instruct = f\"ekrombouts/{nursing_care_home_name}_instruct_dataset\"\n",
    "commit_message = \"Instruct dataset\"\n",
    "\n",
    "# Function to generate file paths for embeddings\n",
    "def get_embedding_path(gender, context_or_instruction):\n",
    "    return os.path.join(base_dir, f'data/care_pal/{gender}_{context_or_instruction}_embeddings.pt')\n",
    "\n",
    "# File paths for saving/loading the embeddings\n",
    "fn_male_context_embeddings = get_embedding_path('male', 'context')\n",
    "fn_female_context_embeddings = get_embedding_path('female', 'context')\n",
    "fn_male_instruction_embeddings = get_embedding_path('male', 'instruction')\n",
    "fn_female_instruction_embeddings = get_embedding_path('female', 'instruction')\n",
    "\n",
    "# File path for saving the generated responses\n",
    "fn_responses = os.path.join(base_dir, 'data/care_pal/context_instruction_pairs_with_responses.pkl')\n",
    "\n",
    "# Additional parameters\n",
    "num_general_prompts = 250\n",
    "k_instructions = 2\n",
    "k_contexts = 50\n",
    "sep_line = 50 * '-'\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DG6Elcx5YdSB"
   },
   "source": [
    "## Loading and Preprocessing Data\n",
    "Client records and notes from fictional clients of a nursing home are loaded, cleaned, and processed. Client genders are identified, and the notes are grouped by week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Vc3BWewlask"
   },
   "outputs": [],
   "source": [
    "# Load datasets from Hugging Face and preprocess\n",
    "def load_and_preprocess_data():\n",
    "    dataset_records = load_dataset(path_hf_records)\n",
    "    dataset_clients = load_dataset(path_hf_clients)\n",
    "\n",
    "    df_records = dataset_records['train'].to_pandas()\n",
    "    df_clients = dataset_clients['train'].to_pandas()\n",
    "\n",
    "    def determine_client_gender(row):\n",
    "        name = row[\"name\"]\n",
    "        if \"Mevrouw\" in name:\n",
    "            return \"female\"\n",
    "        elif \"Meneer\" in name:\n",
    "            return \"male\"\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "\n",
    "    df = (df_records\n",
    "          .dropna()\n",
    "          .assign(week=lambda df: pd.to_datetime(df['datetime']).dt.to_period('W').dt.to_timestamp())  # Add 'week' column\n",
    "          .groupby(['client_id', 'week'])\n",
    "          .agg({'note': lambda x: '\\n'.join(x)})  # Concatenate 'note' values\n",
    "          .reset_index()\n",
    "          .rename(columns={'note': 'weeknotes'})\n",
    "          .merge(df_clients[['client_id', 'name']], on='client_id', how='left')  # Merge with client name\n",
    "          .assign(gender=lambda df: df.apply(determine_client_gender, axis=1))  # Determine gender\n",
    "         )\n",
    "    return df, df_records\n",
    "\n",
    "df, df_records = load_and_preprocess_data()\n",
    "\n",
    "if verbose:\n",
    "  print(f\"Rows in original df: {df_records.shape[0]}, rows in processed df: {df.shape[0]}\\n\")\n",
    "  print(f\"SAMPLES{sep_line}\\n{df.sample(3)}\\n\")\n",
    "  print(f\"\\nContext column (weeknotes) example:{sep_line}\\n{df['weeknotes'].iloc[0]}\")\n",
    "  print(f\"\\nPercentage gender:{sep_line}\\n{df['gender'].value_counts(normalize=True)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0X7X2lFyYRhc"
   },
   "source": [
    "## Instruction design\n",
    "A list of question prompts is created for male, female, and general contexts. These prompts are relevant to nursing home care, focusing on conditions, care requirements, and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZuuYhAgc-SV"
   },
   "outputs": [],
   "source": [
    "# Define instruction prompts for male, female, and general contexts\n",
    "instructions_male = [\n",
    "    \"Beschrijf lichamelijke klachten\",\n",
    "    \"Hoe voelt de patiënt zich?\",\n",
    "    \"Welke ziektes heeft cliënt?\",\n",
    "    \"Beschrijf de klachten van meneer\",\n",
    "    \"Heeft deze cliënt pijn?\",\n",
    "    \"Welke ongemakken ervaart dhr?\",\n",
    "    \"Welke behandeling is ingezet?\",\n",
    "    \"Beschrijf of er wonden of huidproblemen zijn\",\n",
    "    \"Beschrijf de benodigde ADL hulp\",\n",
    "    \"Beschrijf bijzonderheden over eten en drinken\",\n",
    "    \"Welke hulp heeft dhr nodig bij wassen en aankleden?\",\n",
    "    \"Geef aan welke hulp wordt geboden bij eten en drinken\",\n",
    "    \"Wordt meneer geholpen bij douchen?\",\n",
    "    \"Hoe wordt de ADL gedaan?\",\n",
    "    \"Beschrijf de mobiliteit van meneer\",\n",
    "    \"Welk loophulpmiddel gebruikt cliënt?\",\n",
    "    \"Beschrijf de mate van valgevaar\",\n",
    "    \"Welke hulp wordt geboden bij de mobiliteit?\",\n",
    "    \"Beschrijf de daginvulling van meneer\",\n",
    "    \"Doet ct mee aan activiteiten?\",\n",
    "    \"Hoe verlopen de nachten?\",\n",
    "    \"Heeft meneer lekker geslapen?\",\n",
    "    \"Geef aan of er stemmingsklachten zijn\",\n",
    "    \"Beschrijf gedragsproblemen\",\n",
    "    \"Hoe is de cognitie van meneer?\",\n",
    "]\n",
    "\n",
    "instructions_female = [\n",
    "    \"Beschrijf lichamelijke klachten\",\n",
    "    \"Hoe voelt de patiënt zich?\",\n",
    "    \"Welke ziektes heeft cliënte?\",\n",
    "    \"Beschrijf de klachten van mevrouw\",\n",
    "    \"Heeft deze cliënte pijn?\",\n",
    "    \"Welke ongemakken ervaart mw?\",\n",
    "    \"Welke behandeling is ingezet?\",\n",
    "    \"Beschrijf of er wonden of huidproblemen zijn\",\n",
    "    \"Beschrijf de benodigde ADL hulp\",\n",
    "    \"Beschrijf bijzonderheden over eten en drinken\",\n",
    "    \"Welke hulp heeft mw nodig bij wassen en aankleden?\",\n",
    "    \"Geef aan welke hulp wordt geboden bij eten en drinken\",\n",
    "    \"Wordt mevrouw geholpen bij douchen?\",\n",
    "    \"Hoe wordt de ADL gedaan?\",\n",
    "    \"Beschrijf de mobiliteit van mevrouw\",\n",
    "    \"Welk loophulpmiddel gebruikt cliënte?\",\n",
    "    \"Beschrijf de mate van valgevaar\",\n",
    "    \"Welke hulp wordt geboden bij de mobiliteit?\",\n",
    "    \"Beschrijf de daginvulling van mevrouw\",\n",
    "    \"Doet cte mee aan activiteiten?\",\n",
    "    \"Hoe verlopen de nachten?\",\n",
    "    \"Heeft mevrouw lekker geslapen?\",\n",
    "    \"Geef aan of er stemmingsklachten zijn\",\n",
    "    \"Beschrijf gedragsproblemen\",\n",
    "    \"Hoe is de cognitie van mevrouw?\",\n",
    "]\n",
    "\n",
    "instructions_general = [\n",
    "    \"Geef twee belangrijke punten waarop moet worden geobserveerd en gerapporteerd\",\n",
    "    \"Noem de aandachtspunten voor het zorgpersoneel\",\n",
    "    \"Welke acties moet het zorgteam nemen op basis van deze rapportages?\",\n",
    "    \"Vat de rapportages kort en bondig samen\",\n",
    "]\n",
    "\n",
    "# Combine all instruction prompts\n",
    "instructions = instructions_male + instructions_female + instructions_general\n",
    "\n",
    "if verbose:\n",
    "    print(f\"Number of male instructions: {len(instructions_male)}\")\n",
    "    print(f\"Number of female instructions: {len(instructions_female)}\")\n",
    "    print(f\"Number of general instructions: {len(instructions_general)}\")\n",
    "    print(f\"Total number of instructions: {len(instructions)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0A3R8atu72M"
   },
   "outputs": [],
   "source": [
    "# Create lists of contexts\n",
    "male_contexts = df[df['gender'] == 'male']['weeknotes'].tolist()\n",
    "female_contexts = df[df['gender'] == 'female']['weeknotes'].tolist()\n",
    "contexts = male_contexts + female_contexts\n",
    "\n",
    "if verbose:\n",
    "    print(f\"Number of male contexts: {len(male_contexts)}\")\n",
    "    print(f\"Number of female contexts: {len(female_contexts)}\")\n",
    "    print(f\"Total number of contexts: {len(contexts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IA4_NaLw_OzC"
   },
   "source": [
    "## Embedding generation\n",
    "Contexts (notes) and instructions are converted into embeddings using a sentence transformer model. These embeddings are used to represent the semantic meaning of the text and match the instructions to contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjqjRS7PDBLg"
   },
   "outputs": [],
   "source": [
    "# Load the embeddings model\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "\n",
    "# Function to load or generate embeddings\n",
    "def load_or_generate_embeddings(file_path, data, model):\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading embeddings from {file_path}\")\n",
    "        embeddings = torch.load(file_path, weights_only=True)\n",
    "    else:\n",
    "        print(f\"Generating embeddings for {file_path}\")\n",
    "        embeddings = model.encode(\n",
    "            sentences=data,\n",
    "            convert_to_tensor=True,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        torch.save(embeddings, file_path)\n",
    "    return embeddings\n",
    "\n",
    "# Load or generate embeddings for male and female contexts and instructions\n",
    "male_context_embeddings = load_or_generate_embeddings(fn_male_context_embeddings, male_contexts, model)\n",
    "female_context_embeddings = load_or_generate_embeddings(fn_female_context_embeddings, female_contexts, model)\n",
    "male_instruction_embeddings = load_or_generate_embeddings(fn_male_instruction_embeddings, instructions_male, model)\n",
    "female_instruction_embeddings = load_or_generate_embeddings(fn_female_instruction_embeddings, instructions_female, model)\n",
    "\n",
    "if verbose:\n",
    "    print(f\"\\nLength of male_context_embeddings: {len(male_context_embeddings)}\")\n",
    "    print(f\"\\nShape of the first embedding: {male_context_embeddings[0].shape}\")\n",
    "    print(f\"\\nFirst embedding:\\n{male_context_embeddings[0][:20]}...\")  # Only shows the first values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVgs7shlAU_Q"
   },
   "source": [
    "## Matching Instructions to Contexts\n",
    "Using cosine similarity, instructions are paired with the corresponding client notes. This helps in determining which instructions fit which contexts best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OP3rT08xiWo-"
   },
   "outputs": [],
   "source": [
    "# Function to calculate top-k similarities\n",
    "def get_top_k_indices(cosine_sim_matrix, k, dim):\n",
    "    return torch.topk(torch.tensor(cosine_sim_matrix), k=k, dim=dim)\n",
    "\n",
    "def process_context_instruction_pairs(cosine_sim_matrix, contexts, instructions, k_instructions=2, k_contexts=50):\n",
    "    context_instruction_pairs = []\n",
    "\n",
    "    # Top K instructions per context\n",
    "    top_k_instructions_for_contexts = get_top_k_indices(cosine_sim_matrix, k=k_instructions, dim=1)\n",
    "\n",
    "    # Top K contexts per instruction\n",
    "    top_k_contexts_for_instructions = get_top_k_indices(cosine_sim_matrix, k=k_contexts, dim=0)\n",
    "\n",
    "    # Least fitting context per instruction\n",
    "    worst_contexts_for_instructions = torch.argmin(torch.tensor(cosine_sim_matrix), dim=0)\n",
    "\n",
    "    # Add top K instructions for each context\n",
    "    for i, top_instruction_indices in enumerate(top_k_instructions_for_contexts.indices):\n",
    "        for idx in top_instruction_indices:\n",
    "            context_instruction_pairs.append({\n",
    "                \"context\": contexts[i],\n",
    "                \"instruction\": instructions[idx.item()],\n",
    "                \"similarity\": cosine_sim_matrix[i, idx.item()],\n",
    "                \"relationship_type\": \"top instructions for context\"\n",
    "            })\n",
    "\n",
    "    # Add top K contexts for each instruction\n",
    "    for j, top_context_indices in enumerate(top_k_contexts_for_instructions.indices.T):\n",
    "        for idx in top_context_indices:\n",
    "            context_instruction_pairs.append({\n",
    "                \"context\": contexts[idx.item()],\n",
    "                \"instruction\": instructions[j],\n",
    "                \"similarity\": cosine_sim_matrix[idx.item(), j],\n",
    "                \"relationship_type\": \"top contexts for instruction\"\n",
    "            })\n",
    "\n",
    "    # Add least fitting context for each instruction\n",
    "    for j, worst_context_idx in enumerate(worst_contexts_for_instructions):\n",
    "        context_instruction_pairs.append({\n",
    "            \"context\": contexts[worst_context_idx.item()],\n",
    "            \"instruction\": instructions[j],\n",
    "            \"similarity\": cosine_sim_matrix[worst_context_idx.item(), j],\n",
    "            \"relationship_type\": \"worst context for instruction\"\n",
    "        })\n",
    "\n",
    "    return context_instruction_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yN56nmeOibPH"
   },
   "outputs": [],
   "source": [
    "# Process male and female datasets\n",
    "male_cosine_sim_matrix = cosine_similarity(male_context_embeddings, male_instruction_embeddings)\n",
    "female_cosine_sim_matrix = cosine_similarity(female_context_embeddings, female_instruction_embeddings)\n",
    "\n",
    "context_instruction_pairs_male = process_context_instruction_pairs(\n",
    "    cosine_sim_matrix=male_cosine_sim_matrix,\n",
    "    contexts=male_contexts, instructions=instructions_male,\n",
    "    k_instructions=k_instructions,\n",
    "    k_contexts=k_contexts\n",
    ")\n",
    "\n",
    "context_instruction_pairs_female = process_context_instruction_pairs(\n",
    "    cosine_sim_matrix=female_cosine_sim_matrix,\n",
    "    contexts=female_contexts,\n",
    "    instructions=instructions_female,\n",
    "    k_instructions=k_instructions,\n",
    "    k_contexts=k_contexts)\n",
    "\n",
    "# Combine male and female pairs\n",
    "context_instruction_pairs = context_instruction_pairs_male + context_instruction_pairs_female\n",
    "\n",
    "# Add general instructions to context-instruction pairs\n",
    "random.seed(seed)\n",
    "for instruction in instructions_general:\n",
    "    sampled_contexts = random.sample(contexts, num_general_prompts)\n",
    "    for context in sampled_contexts:\n",
    "        context_instruction_pairs.append({\n",
    "            \"context\": context,\n",
    "            \"instruction\": instruction,\n",
    "            \"similarity\": 0.0,\n",
    "            \"relationship_type\": \"general\"\n",
    "        })\n",
    "\n",
    "if verbose:\n",
    "    print(f\"Cosine similarity matrix shape - male: {male_cosine_sim_matrix.shape}\")\n",
    "    print(f\"Cosine similarity matrix shape - female: {female_cosine_sim_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEuMwTV7llZo"
   },
   "outputs": [],
   "source": [
    "# Convert context_instruction_pairs into a DataFrame\n",
    "df_context_instruction_pairs = pd.DataFrame(context_instruction_pairs)\n",
    "\n",
    "if verbose:\n",
    "    print(f\"SAMPLES\\n{df_context_instruction_pairs.sample(3)}\\n\")\n",
    "    print(\"INFO\")\n",
    "    print(df_context_instruction_pairs.info())\n",
    "    print(f\"\\nVALUE COUNTS\\n{df_context_instruction_pairs['instruction'].value_counts()}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LG4KkcOhmW90"
   },
   "outputs": [],
   "source": [
    "# Remove duplicates of columns 'context', 'instruction' en 'similarity'\n",
    "df_context_instruction_pairs = df_context_instruction_pairs.drop_duplicates(subset=['context', 'instruction', 'similarity'])\n",
    "\n",
    "if verbose:\n",
    "    print(f\"Num rows after dropping duplicates: {df_context_instruction_pairs.shape[0]}\\n\")\n",
    "    print(\"Info\")\n",
    "    print(df_context_instruction_pairs.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApvorATGc4Ar"
   },
   "source": [
    "## LLM Response Generation\n",
    "An llm is used to generate answers to the paired instructions based on the context provided. The generated responses are stored in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIX1bg0nqnCd"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Lees onderstaande rapportages, die een periode van een client in het verpleeghuis beschrijven, en beantwoord onderstaande instructie.\n",
    "Baseer je uitsluitend op de informatie die in de rapportages staat. Als er geen relevante informatie in staat, zeg dat dan. Hou je antwoord kort en bondig.\n",
    "\n",
    "RAPPORTAGES:\n",
    "{context}\n",
    "\n",
    "INSTRUCTIE:\n",
    "{instruction}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"instruction\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "# Initialize the language model with specified parameters\n",
    "llm = ChatOpenAI(\n",
    "    api_key=setup.get_openai_key(),\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    temperature=0.3,\n",
    "    presence_penalty=0.2,\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm\n",
    "\n",
    "if verbose:\n",
    "    sample_id = 50\n",
    "    sample_context = df_context_instruction_pairs['context'].iloc[sample_id]\n",
    "    sample_instruction = df_context_instruction_pairs['instruction'].iloc[sample_id]\n",
    "\n",
    "    sample_prompt = template.format(\n",
    "            context=sample_context,\n",
    "            instruction=sample_instruction\n",
    "    )\n",
    "\n",
    "    result = chain.invoke({\"context\": sample_context, \"instruction\": sample_instruction})\n",
    "\n",
    "    print(sample_prompt)\n",
    "    print(\"RELATIONSHIP TYPE\")\n",
    "    print(df_context_instruction_pairs['relationship_type'].iloc[sample_id])\n",
    "    print(\"RESPONSE\")\n",
    "    print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UN_H58ITyuQo"
   },
   "outputs": [],
   "source": [
    "# Load the previously saved dataframe if it exists, otherwise start fresh\n",
    "if os.path.exists(fn_responses):\n",
    "    df_context_instruction_pairs = pd.read_pickle(fn_responses)\n",
    "else:\n",
    "    df_context_instruction_pairs['llm_response'] = None  # Ensure the column exists\n",
    "\n",
    "# Function to add LLM answer to the dataframe with error handling\n",
    "def get_llm_answer(row, cb):\n",
    "    try:\n",
    "        if pd.isna(row['llm_response']):  # Process only new prompts\n",
    "            result = chain.invoke({\"context\": row['context'], \"instruction\": row['instruction']}, callbacks=[cb])\n",
    "            return result.content\n",
    "        else:\n",
    "            return row['llm_response']  # Keep existing answers\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {row.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create a callback instance to track cost\n",
    "with get_openai_callback() as cb:\n",
    "\n",
    "    # Iterate over the rows and save progress intermittently\n",
    "    for idx, row in df_context_instruction_pairs.iterrows():\n",
    "        df_context_instruction_pairs.at[idx, 'llm_response'] = get_llm_answer(row, cb)\n",
    "\n",
    "        # Save progress every 10 iterations\n",
    "        if idx % 100 == 0:\n",
    "            df_context_instruction_pairs.to_pickle(fn_responses)\n",
    "            print(f\"Checkpoint saved at index {idx}, total cost so far: ${cb.total_cost:.4f}\")\n",
    "\n",
    "    # Save the final result\n",
    "    df_context_instruction_pairs.to_pickle(fn_responses)\n",
    "    print(\"Processing complete and final dataframe saved.\")\n",
    "    print(f\"Total cost: ${cb.total_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THBYapq8g4gh"
   },
   "outputs": [],
   "source": [
    "example_ct = 63\n",
    "print(df_context_instruction_pairs['prompt'].iloc[example_ct])\n",
    "print(\"\\nRESPONSE:\")\n",
    "print(df_context_instruction_pairs['llm_response'].iloc[example_ct])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BbAG4vGhAe1"
   },
   "source": [
    "## Dataset Creation, Splitting, and Saving\n",
    "The context-instruction-response pairs are compiled into a single dataset. This dataset is then split into training, validation, and test sets and finally, the dataset is saved locally and prepared for uploading to Hugging Face, allowing it to be shared or reused in future projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vkm-jb-1Y5SZ"
   },
   "outputs": [],
   "source": [
    "df = (df_context_instruction_pairs\n",
    "      .loc[:, ['context', 'instruction', 'llm_response']]\n",
    "      .rename(columns={'llm_response': 'response'})\n",
    "     )\n",
    "\n",
    "# Convert df to Hugging Face dataset\n",
    "dataset = Dataset.from_pandas(\n",
    "    df=df,\n",
    "    preserve_index=False\n",
    ")\n",
    "\n",
    "# Split the dataset into training and test/validation splits\n",
    "train_testvalid_split = dataset.train_test_split(test_size=0.2, seed=seed)\n",
    "\n",
    "# Further split the test set into validation and test sets\n",
    "test_valid_split = train_testvalid_split['test'].train_test_split(test_size=0.5, seed=seed)\n",
    "\n",
    "# Create a DatasetDict object to hold the splits\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_testvalid_split['train'],\n",
    "    'validation': test_valid_split['train'],\n",
    "    'test': test_valid_split['test'],\n",
    "})\n",
    "\n",
    "# # Push the dataset to HuggingFace Hub with the specified path and commit message\n",
    "# dataset_dict.push_to_hub(path_hf_instruct,\n",
    "#                          commit_message=commit_message,\n",
    "#                          private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIysUnOGkQxL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gcai_zuster_fietje",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
