{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekrombouts/gcai_zuster_fietje/blob/main/notebooks/310_GenCareAIFietjePeftFinetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ-jErPJmWy4"
      },
      "source": [
        "# Zuster Fietje, PEFT finetuning ekrombouts/zuster_fietje\n",
        "\n",
        "**Author:** Eva Rombouts  \n",
        "**Date:** 2024-07-28  \n",
        "**Updated:** 2024-10-18\n",
        "\n",
        "### Description\n",
        "This notebook is almost fully copied from: [Optimizing Phi-2: A Deep Dive into Fine-Tuning Small Language Models](https://medium.com/thedeephub/optimizing-phi-2-a-deep-dive-into-fine-tuning-small-language-models-9d545ac90a99), by Praveen Yerneni. Thank you!!\n",
        "It trains the chat version of [Fietje](https://huggingface.co/BramVanroy/fietje-2-chat), an adapated version of microsoft/phi-2, trained on Dutch texts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyLKlyMkORZO"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQxgMZ32pzx-"
      },
      "outputs": [],
      "source": [
        "!pip install -q bitsandbytes flash_attn datasets peft\n",
        "\n",
        "verbose = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpsFD-hEMMo6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "\n",
        "from google.colab import drive, runtime\n",
        "import time\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "\n",
        "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
        "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZWtnR2WMPR-"
      },
      "outputs": [],
      "source": [
        "# The base model from Hugging Face that will be finetuned\n",
        "# base_model = \"BramVanroy/fietje-2-instruct\"\n",
        "base_model = \"ekrombouts/zuster_fietje\"\n",
        "\n",
        "# The name of the finetuned model to be saved\n",
        "finetuned_model = \"zuster_fietje_peft\"\n",
        "\n",
        "# Commit message for version control\n",
        "commit_message = \"full finetuned on Gardenia_instruct_dataset for 10 epochs, now PEFT finetuned on Olympia_SAMPC_dataset for 5 epochs\"\n",
        "\n",
        "# Path to the dataset on Hugging Face that will be used for finetuning\n",
        "path_dataset = \"ekrombouts/Olympia_SAMPC_dataset\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfTAS_SRfJ6y"
      },
      "source": [
        "## Load model and tokenizer\n",
        "\n",
        "The model is loaded in `4-bit` which is the \"Quantization\" part of QLORA. The memory footprint of this is much smaller then the default.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCpMhnh0hh0t"
      },
      "outputs": [],
      "source": [
        "# Configuration to load model in 4-bit quantized\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_compute_dtype='float16',\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "\n",
        "#Loading the model with compatible settings\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    device_map='auto',\n",
        "    quantization_config=bnb_config,\n",
        "    attn_implementation='flash_attention_2',\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Setting up the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model,\n",
        "    add_eos_token=True,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.truncation_side = 'left'\n",
        "\n",
        "if verbose:\n",
        "    print(f\"Memory footprint: {model.get_memory_footprint() / 1e9} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teSp0jIXzedK"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "dataset = load_dataset(path_dataset)\n",
        "train_dataset = dataset['train']\n",
        "val_dataset = dataset['validation']\n",
        "\n",
        "if verbose:\n",
        "    print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt(row: dict, add_response: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Generates a prompt based on the input data in 'row'.\n",
        "\n",
        "    Args:\n",
        "        row (dict): A dictionary containing 'context', 'instruction', and optionally 'response'.\n",
        "        full (bool): If True, the prompt will include the 'response'.\n",
        "                     If False, only 'context' and 'instruction' will be included.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated prompt in text format.\n",
        "    \"\"\"\n",
        "    # Base prompt (without response)\n",
        "    prompt = f\"\"\"Context:\n",
        "{row['context']}\n",
        "\n",
        "Instructie:\n",
        "{row['instruction']}\n",
        "\n",
        "Antwoord:\"\"\"\n",
        "    # Append response if 'add_response' is True\n",
        "    if add_response:\n",
        "        prompt += f\"\\n{row['response']}\\n\"\n",
        "\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "4CjWBpP92V96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a random example of the model's output before training\n",
        "if verbose:\n",
        "    import random\n",
        "    row = random.choice(train_dataset)  # Select a random row from the training dataset\n",
        "    prompt = create_prompt(row, False)  # Create the prompt from the selected dataset row\n",
        "    print(prompt)\n",
        "\n",
        "    # Convert the prompt into tokens that the model can understand\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "    attention_mask = tokenizer(prompt, return_tensors=\"pt\", padding=True).attention_mask.to(model.device)\n",
        "\n",
        "    # Enable the model's cache for faster generation and switch to evaluation mode\n",
        "    model.config.use_cache = True\n",
        "    model.eval()\n",
        "\n",
        "    # Generate a response based on the input prompt\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=150,  # Limit the number of new tokens generated to 150\n",
        "        do_sample=True,      # Use sampling to introduce randomness into the generation\n",
        "        top_p=0.95,          # Use nucleus sampling with a probability threshold of 0.95\n",
        "        top_k=50,            # Consider the top 50 tokens when sampling for each step\n",
        "        temperature=0.7,     # Set the temperature to 0.7 to control randomness (lower = more conservative)\n",
        "        num_return_sequences=1,  # Generate only one sequence\n",
        "        eos_token_id=tokenizer.eos_token_id,  # End the generation when the EOS token is reached\n",
        "        pad_token_id=tokenizer.eos_token_id   # Use the EOS token for padding\n",
        "    )\n",
        "\n",
        "    # Convert the generated token sequence back into text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    generated_response = generated_text[len(prompt):].strip()  # Remove the prompt part from the output\n",
        "\n",
        "    # Display the generated response and the actual reference response from the dataset\n",
        "    print(\"GENERATED RESPONSE:\")\n",
        "    print(generated_response)\n",
        "    print(\"\\nREFERENCE RESPONSE:\")\n",
        "    print(row['response'])\n"
      ],
      "metadata": {
        "id": "6Jhs4TlNNi05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjiMMZ-gkotJ"
      },
      "outputs": [],
      "source": [
        "def collate_and_tokenize(row):\n",
        "    \"\"\"\n",
        "    Tokenizes and prepares a dataset sample for training.\n",
        "\n",
        "    Args:\n",
        "        row (dict): A single row or sample from the dataset, typically containing\n",
        "                    input text fields.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing tokenized input tensors and labels, with keys:\n",
        "              - 'input_ids': Tokenized input IDs for the model.\n",
        "              - 'attention_mask': Attention mask indicating which tokens should be attended to.\n",
        "              - 'labels': Tokenized labels for model training, identical to input_ids.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the prompt from the dataset row\n",
        "    prompt = create_prompt(\n",
        "        row=row,\n",
        "        add_response=True,\n",
        "    )\n",
        "\n",
        "    # Tokenize the prompt and prepare input tensors\n",
        "    encoded = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",   # Return tensors in PyTorch format\n",
        "        padding=\"max_length\",  # Pad the input to the maximum length\n",
        "        truncation=True,       # Truncate inputs that are longer than the max length\n",
        "        max_length=2048,       # Set the maximum length for input tokens\n",
        "    )\n",
        "\n",
        "    # Create labels by duplicating input IDs for the model to predict\n",
        "    encoded[\"labels\"] = encoded[\"input_ids\"].clone()\n",
        "\n",
        "    return encoded  # Return the tokenized data with labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZLVRL2Wk3vj"
      },
      "outputs": [],
      "source": [
        "#We will just keep the input_ids and labels that we add in function above.\n",
        "columns_to_remove = ['client_id', 'week', 'context', 'instruction', 'response']\n",
        "\n",
        "#tokenize the training and validation datasets\n",
        "tokenized_dataset_train = train_dataset.map(\n",
        "    collate_and_tokenize,\n",
        "    batched=True,\n",
        "    batch_size=1,\n",
        "    remove_columns=columns_to_remove\n",
        ")\n",
        "\n",
        "tokenized_dataset_val = val_dataset.map(\n",
        "    collate_and_tokenize,\n",
        "    batched=True,\n",
        "    batch_size=1,\n",
        "    remove_columns=columns_to_remove\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tez7pYN4jqTF"
      },
      "outputs": [],
      "source": [
        "if verbose:\n",
        "    #Check if tokenization looks good\n",
        "    input_ids = tokenized_dataset_val[1]['input_ids']\n",
        "\n",
        "    decoded = tokenizer.decode(\n",
        "        input_ids,\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    print(decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vB9NMBzasFcT"
      },
      "outputs": [],
      "source": [
        "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
        "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        ")\n",
        "\n",
        "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dipMfTbgPvoM"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uamQX0JVvoGF"
      },
      "outputs": [],
      "source": [
        "if verbose:\n",
        "    print_trainable_parameters(model)\n",
        "\n",
        "#gradient checkpointing to save memory\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Freeze base model layers and cast layernorm in fp32\n",
        "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
        "if verbose:\n",
        "    print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PuVk7YBwxY9"
      },
      "outputs": [],
      "source": [
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\n",
        "    'q_proj',\n",
        "    'k_proj',\n",
        "    'v_proj',\n",
        "    'dense',\n",
        "    'fc1',\n",
        "    'fc2',\n",
        "    ], #print(model) will show the modules to use\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(model, config)\n",
        "if verbose:\n",
        "    print_trainable_parameters(lora_model)\n",
        "\n",
        "lora_model = accelerator.prepare_model(lora_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2T1Q6Zz9k3H"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/results_full',  # Directory where the model checkpoints and outputs will be saved\n",
        "    report_to='none',\n",
        "    overwrite_output_dir=True, # Overwrite the content of the output directory\n",
        "    per_device_train_batch_size=8,  # Batch size for training\n",
        "    per_device_eval_batch_size=8,  # Batch size for evaluation\n",
        "    gradient_accumulation_steps=5, # number of steps before optimizing\n",
        "    gradient_checkpointing=True,   # Enable gradient checkpointing\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    warmup_steps=50,  # Number of warmup steps\n",
        "    num_train_epochs=5,  # Number of training epochs\n",
        "    learning_rate=5e-5,  # Learning rate\n",
        "    weight_decay=0.01,  # Weight decay\n",
        "    optim=\"paged_adamw_8bit\", #Keep the optimizer state and quantize it\n",
        "    fp16=True, #Use mixed precision training\n",
        "    #For logging and saving\n",
        "    logging_dir='/content/drive/MyDrive/logs',  # Directory for saving logs\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    save_total_limit=2,  # Limit the total number of checkpoints\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    load_best_model_at_end=True, # Load the best model at the end of training\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    train_dataset=tokenized_dataset_train,\n",
        "    eval_dataset=tokenized_dataset_val,\n",
        "    args=training_args,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Disable cache to prevent warning, reenable for inference\n",
        "model.config.use_cache = False\n",
        "\n",
        "start_time = time.time()\n",
        "trainer.train()\n",
        "end_time = time.time()\n",
        "\n",
        "training_time = end_time - start_time\n",
        "\n",
        "print(f\"Training completed in {training_time} seconds.\")\n"
      ],
      "metadata": {
        "id": "EX7rwbm922Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save model to hub to ensure we save our work.\n",
        "lora_model.push_to_hub(\n",
        "    finetuned_model,\n",
        "    use_auth_token=True,\n",
        "    commit_message=commit_message,\n",
        "    private=True\n",
        ")\n",
        "\n",
        "tokenizer.push_to_hub(\n",
        "    finetuned_model,\n",
        "    use_auth_token=True,\n",
        "    commit_message=commit_message,\n",
        "    private=True\n",
        ")"
      ],
      "metadata": {
        "id": "DmTQetP02498"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Terminate the session so we do not incur cost\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "5rGe0ZEb27kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "12gL2SEJ3AVQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}