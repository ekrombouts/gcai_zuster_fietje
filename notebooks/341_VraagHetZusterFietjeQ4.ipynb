{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekrombouts/gcai_zuster_fietje/blob/main/notebooks/341_VraagHetZusterFietjeQ4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets bitsandbytes\n"
      ],
      "metadata": {
        "id": "v-XQ3KLsg9fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAAPAAhDOPRj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orbb3oKWOPLZ"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained fine-tuned model and tokenizer\n",
        "model_id = \"ekrombouts/zuster_fietje_q4\"\n",
        "peft_model_id = \"ekrombouts/zuster_fietje_peft_q4\"\n",
        "\n",
        "# Load the base model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.truncation_side = 'left'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WHFpzUIOPGV"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "path_hf_sampc = \"ekrombouts/Gardenia_instruct_dataset\"\n",
        "dataset = load_dataset(path_hf_sampc)\n",
        "# We will be working with the validation dataset\n",
        "val_dataset = dataset['validation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5XYyn0tQpRE"
      },
      "outputs": [],
      "source": [
        "def create_prompt(row: dict, add_response: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Generates a prompt based on the input data in 'row'.\n",
        "\n",
        "    Args:\n",
        "        row (dict): A dictionary containing 'context', 'instruction', and optionally 'response'.\n",
        "        add_response (bool): If True, the prompt will include the 'response'.\n",
        "                             If False, only 'context' and 'instruction' will be included.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated prompt in text format.\n",
        "    \"\"\"\n",
        "    # Base prompt (without response)\n",
        "    prompt = f\"\"\"|CONTEXT|\n",
        "{row.get('context', '')}\n",
        "\n",
        "|INSTRUCTION|\n",
        "{row.get('instruction', '')}\n",
        "\n",
        "|RESPONSE|\n",
        "\"\"\"\n",
        "\n",
        "    # Append response if 'add_response' is True and 'response' exists\n",
        "    if add_response and 'response' in row:\n",
        "        prompt += f\"\\n{row['response']}\\n\"\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uVHKvxmnExH"
      },
      "outputs": [],
      "source": [
        "def tokenize_prompt(model, prompt):\n",
        "    # Tokenize and prepare input\n",
        "    return tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device), \\\n",
        "           tokenizer(prompt, return_tensors=\"pt\", padding=True).attention_mask.to(model.device)\n",
        "\n",
        "def generate_output(model, input_ids, attention_mask):\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=1024,\n",
        "            do_sample=True,\n",
        "            top_p=1,\n",
        "            top_k=50,\n",
        "            temperature=0.1,\n",
        "            num_return_sequences=1,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "def answer(model, prompt):\n",
        "    input_ids, attention_mask = tokenize_prompt(model, prompt)\n",
        "    generated_text = generate_output(\n",
        "        model=model,\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask)\n",
        "    return generated_text[len(prompt):].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxgAZuTgOpHK"
      },
      "outputs": [],
      "source": [
        "# Prepare the prompt with notes from sample\n",
        "# row = val_dataset[1]\n",
        "row = random.choice(val_dataset)  # Select a random row from the training dataset\n",
        "\n",
        "prompt = create_prompt(\n",
        "    row=row,\n",
        "    add_response=False\n",
        ")\n",
        "print(prompt)\n",
        "\n",
        "# Display the generated response and actual response\n",
        "ref_response = row['response']  # Reference response from dataset\n",
        "print(\"\\nREFERENCE RESPONSE:\")\n",
        "print(ref_response)\n",
        "\n",
        "print(f\"\\n{100*'-'}\")\n",
        "print(\"ZUSTER FIETJE:\")\n",
        "print(answer(model,prompt))\n",
        "print(f\"\\n{100*'-'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5qUhe8Tf1ok"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"|CONTEXT|\n",
        "\n",
        "U was inc van urine. U was niet vriendelijk tijdens het verschonen.\n",
        "Mw was vanmorgen incontinent van dunne def, bed was ook nat. Mw is volledig verzorgd, bed is verschoond,\n",
        "Mw. haar kledingkast is opgeruimd.\n",
        "Mw. zei:\"oooh kind, ik heb zo'n pijn. Mijn benen. Dat gaat nooit meer weg.\" Mw. zat in haar rolstoel en haar gezicht trok weg van de pijn en kreeg traanogen. Mw. werkte goed mee tijdens adl. en was vriendelijk aanwezig. Pijn. Mw. kreeg haar medicatie in de ochtend, waaronder pijnstillers. 1 uur later adl. gegeven.\n",
        "Ik lig hier maar voor Piet Snot. Mw. was klaarwakker tijdens eerste controle. Ze wilde iets, maar wist niet wat. Mw. een slokje water gegeven en uitgelegd hoe ze kon bellen als ze iets wilde. Mw. pakte mijn hand en bedankte me.\n",
        "Mevr. in de ochtend ondersteund met wassen en aankleden. Mevr was rustig aanwezig.\n",
        "Mw is volledig geholpen met ochtendzorg, mw haar haren zijn gewassen. Mw haar nagels zijn kort geknipt.\n",
        "Mevr heeft het ontbijt op bed genuttigd. Daarna mocht ik na de tweede poging Mevr ondersteunen met wassen en aankleden.\n",
        "Vanmorgen met mw naar buiten geweest om een sigaret te roken. Mw was niet erg spraakzaam en mw kwam op mij over alsof ze geen behoefte had aan een gesprek. Mw kreeg het koud door de wind en wilde snel weer naar binnen.\n",
        "\n",
        "|INSTRUCTION|\n",
        "Noem de 3 belangrijkste zorgproblemen\n",
        "\n",
        "|RESPONSE|\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4SjVnY0dJnh"
      },
      "outputs": [],
      "source": [
        "print(\"ZUSTER FIETJE:\")\n",
        "print(answer(model,prompt))\n",
        "print(f\"\\n{100*'-'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZtU77bylO_E"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}